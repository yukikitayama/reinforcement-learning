# -*- coding: utf-8 -*-
"""ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eoDwZt2_hfwBHymL0dAukEfyXKZL_x40
"""

# Proximal Policy Optimization

## setup

import gym

import numpy as np
import matplotlib.pyplot as plt

from keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Flatten
from tensorflow.keras.optimizers import RMSprop

## hyperparameter

ENV = 'Pong-v0'

LOSS_CLIPPING = 0.2 # epsilon in PPO paper
ENTROPY_LOSS = 5e-3

plt.style.use('default')

## environment

# https://gym.openai.com/docs/

env = gym.make(ENV)

# atari shared action and space space
print('action space', env.action_space)
print('meaning of action space', env.get_action_meanings())
print('observation space', env.observation_space)

# show environment
# obs = env.reset()
# plt.imshow(obs)
plt.imshow(env.render('rgb_array'))
plt.axis('off')
plt.show()

## clipped surrogate objective

# example
a = 1
r = 0.8
e = 0.1
a0 = r * a
a1 = np.clip(r, 1-e, 1+e) * a
print(a0, a1)
np.min([a0, a1])

# test L^{CLIP}
def l_clip(a, r, e):
    return np.min([r * a, np.clip(r, 1-e, 1+e) * a])

print(l_clip(1, 1.2, 0.1))
print(l_clip(1, 0.8, 0.1))
print(l_clip(-1, 1.2, 0.1))
print(l_clip(-1, 0.8, 0.1))

## proximal policy optimization loss

def ppo_loss(y_true, y_pred):
    # what is the shape of y_true and y_pred?
    advantages = y_true[:, :1]
    prediction_picks = y_true[:, 1:1 + action_space]
    # shape?
    actions = y_ture[:, 1 + action_space:]

    prob = y_pred * actions
    old_prob = actions * prediction_picks
    # why adding?
    r = prob / (old_prob + 1e-10)
    # first term in min of L^CLIP in paper equation (7)
    p1 = r * advantage
    # second term in min of L^CIP in paper equation (7)
    p2 = K.clip(r, min_value = 1 - LOSS_CLIPPING, max_value = 1 + LOSS_CLIPPING) * advantages
    # what is this?
    loss = - K.mean(K.minimum(p1, p2) + ENTROPY_LOSS * - (prob * K.log(prob + 1e-10)))
    return loss

## model

def MakeModels(input_shape, action_space, lr):
    # shared structure for actor and critic
    X_input = Input(input_shape)
    X = Flatten(input_shape = input_shape)(X_input)
    # elu is Exponential Linear Unit
    # what is he_uniform?
    X = Dense(512, activation = 'elu', kernel_initializer = 'he_uniform')(X)
    
    # actor output
    action = Dense(action_space, activation = 'softmax', kernel_initializer = 'he_uniform')(X)
    # critic output
    value = Dense(1, kernel_initializer = 'he_uniform')(X)

    ## use proximal polixy optimization loss to actor

    # make actor
    Actor = Model(inptus = X_input, outputs = action)
    # categorical_crossentropy is replaced with PPO
    Actor.compile(loss = ppo_loss, optimizer = RMSprop(lr = lr))
    # make critic
    Critic = Model(inputs = X_input, outputs = value)
    Critic.compile(loss = 'mse', optimizer = RMSprop(lr = lr))

    return Actor, Critic

