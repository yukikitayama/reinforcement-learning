{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JZjZjRDsGyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## resource\n",
        "\n",
        "# A3C\n",
        "# https://blog.tensorflow.org/2018/07/deep-reinforcement-learning-keras-eager-execution.html\n",
        "# https://github.com/tensorflow/models/blob/master/research/a3c_blogpost/a3c_cartpole.py\n",
        "\n",
        "# thread\n",
        "# https://realpython.com/intro-to-python-threading/\n",
        "# https://stackoverflow.com/questions/59633435/threading-in-jupyter-notebook\n",
        "\n",
        "# env.render() in colab\n",
        "# https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt06y0ClQ4g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## setup\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# env.render\n",
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "# FIFO queue\n",
        "from queue import Queue\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras import layers\n",
        "import os\n",
        "import threading\n",
        "import multiprocessing\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "# evaluation video\n",
        "import imageio\n",
        "# import base64\n",
        "# import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSCnaMXRRD5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## hyperparameter\n",
        "\n",
        "ENV = 'CartPole-v0'\n",
        "SAVE_DIR = '/tmp'\n",
        "VIDEO_DIR_01 = '/tmp/video_random.mp4'\n",
        "VIDEO_DIR_02 = '/tmp/vdieo_a3c.mp4'\n",
        "\n",
        "plt.style.use('default')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZQMyqROobc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7a81bafb-e8f1-46f0-e312-abdc36889c1d"
      },
      "source": [
        "## test threading\n",
        "\n",
        "def myfunc(name):\n",
        "    print(f'myfunc started with {name}')\n",
        "    time.sleep(2)\n",
        "    print('myfunc ended')\n",
        "\n",
        "print('main started')\n",
        "t = threading.Thread(target = myfunc, args = ['realpython'])\n",
        "t.start()\n",
        "print('main ended')\n",
        "time.sleep(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main started\n",
            "myfunc started with realpython\n",
            "main ended\n",
            "myfunc ended\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6f2EINIrPeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b152aec8-ee67-4adf-c9ac-872c7941aeba"
      },
      "source": [
        "## test multiprocessing\n",
        "\n",
        "print(multiprocessing.cpu_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SarhyK4CWlqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## test argparse\n",
        "\n",
        "# parser = argparse.ArgumentParser(\n",
        "#     description = 'Run A3C algorithm on the game '\n",
        "#                   'Cartpole.')\n",
        "# parser.add_argument('--algorithm',\n",
        "#                     default = 'a3c',\n",
        "#                     type = str,\n",
        "#                     help = 'Choose between \\'a3c\\' and \\'random\\'.')\n",
        "# parser.add_argument('--train',\n",
        "#                     dest = 'train',\n",
        "#                     action = 'store_true',\n",
        "#                     help = 'Train our model.')\n",
        "# parser.add_argument('--lr',\n",
        "#                     default = 0.001,\n",
        "#                     help = 'Learning rate for the shared optimizer.')\n",
        "# parser.add_argument('--update-freq',\n",
        "#                     default = 20,\n",
        "#                     type = int,\n",
        "#                     help = 'How often to update the global model.')\n",
        "# parser.add_argument('--max-eps',\n",
        "#                     default = 1000,\n",
        "#                     type = int,\n",
        "#                     help = 'Global maximum number of episodes to run.')\n",
        "# parser.add_argument('--gamma',\n",
        "#                     default = 0.99,\n",
        "#                     help = 'Discount factor of rewards.')\n",
        "# parser.add_argument('--save_dir',\n",
        "#                     default = '/tmp/',\n",
        "#                     type = str,\n",
        "#                     help = 'Directory in which you desire to save the model.')\n",
        "# args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kveyO-P6ZGNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## class argument\n",
        "\n",
        "class Argument:\n",
        "    def __init__(self):\n",
        "        self.algorithm = 'a3c'\n",
        "        self.train = True\n",
        "        self.lr = 0.001\n",
        "        self.update_freq = 20\n",
        "        # self.max_eps = 1000\n",
        "        # self.max_eps = 700\n",
        "        self.max_eps = 600\n",
        "        self.gamma = 0.99\n",
        "        self.save_dir = '/tmp/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8TWIMmOZu-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "82b5c551-f141-4e53-ef74-476394a9f6ee"
      },
      "source": [
        "## test class argument\n",
        "\n",
        "args = Argument()\n",
        "print(args)\n",
        "print(args.save_dir)\n",
        "print(args.lr)\n",
        "print(args.algorithm)\n",
        "print(args.max_eps)\n",
        "print(args.update_freq)\n",
        "print(args.gamma)\n",
        "print(args.train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Argument object at 0x7f4a3df96c18>\n",
            "/tmp/\n",
            "0.001\n",
            "a3c\n",
            "600\n",
            "20\n",
            "0.99\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cPJOUrGHH0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "dc7040ca-c2e0-445f-8e9a-17dff437bb4d"
      },
      "source": [
        "## environment\n",
        "\n",
        "env = gym.make(ENV)\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.shape[0])\n",
        "print(env.action_space)\n",
        "print(env.action_space.n)\n",
        "\n",
        "obs = env.reset()\n",
        "print('obs', obs)\n",
        "action = env.action_space.sample()\n",
        "print('action', action)\n",
        "obs, reward, done, info = env.step(action)\n",
        "print('obs', obs)\n",
        "print('reward', reward)\n",
        "print('done', done)\n",
        "print('info', info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(4,)\n",
            "4\n",
            "Discrete(2)\n",
            "2\n",
            "obs [-0.03847262 -0.00340432  0.00427055  0.02564328]\n",
            "action 1\n",
            "obs [-0.0385407   0.19165613  0.00478341 -0.26568919]\n",
            "reward 1.0\n",
            "done False\n",
            "info {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPmUX1hpIQd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "337fb616-7e71-4541-9de9-49b706f63174"
      },
      "source": [
        "env = gym.make(ENV)\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "plt.show()\n",
        "\n",
        "steps = 50\n",
        "\n",
        "for _ in range(steps):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    screen = env.render(mode = 'rgb_array')\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    # wait = True waits to clear the output until new output is available to replace it\n",
        "    ipythondisplay.clear_output(wait = True)\n",
        "    # gcf() gets the current figure\n",
        "    ipythondisplay.display(plt.gcf())\n",
        " \n",
        "    if done:\n",
        "        break\n",
        "\n",
        "ipythondisplay.clear_output(wait = True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaZVP7BdNRnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "99986be0-c4e2-4ca1-8414-38045abf00e5"
      },
      "source": [
        "## video of random sampling\n",
        "\n",
        "fps = 30\n",
        "steps = 200\n",
        "\n",
        "# get_writer returns a Writer object which can be used to write data and meta data to specified file\n",
        "with imageio.get_writer(VIDEO_DIR_01, fps = fps) as video:\n",
        "    for _ in range(steps):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        screen = env.render(mode = 'rgb_array')\n",
        "        # append_data appends an image (and meta data) to the file\n",
        "        video.append_data(screen)\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            screen = env.render(mode = 'rgb_array')\n",
        "            video.append_data(screen)\n",
        "            state = next_state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTwBoEtSYxRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## global network\n",
        "\n",
        "class ActorCriticModel(keras.Model):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # subclassing the Model class https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "        # define layers in __init__ and implement forward pass in call\n",
        "        super(ActorCriticModel, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.aciton_size = action_size\n",
        "        self.dense1 = layers.Dense(100, activation = 'relu')\n",
        "        self.policy_logits = layers.Dense(action_size)\n",
        "        self.dense2 = layers.Dense(100, activation = 'relu')\n",
        "        self.values = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # forward pass\n",
        "        # policy probability logits, which will be probability later by tf.nn.softmax\n",
        "        x = self.dense1(inputs)\n",
        "        logits = self.policy_logits(x)\n",
        "        # values\n",
        "        v1 = self.dense2(inputs)\n",
        "        values = self.values(v1)\n",
        "        return logits, values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxg353zRws-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## helper function\n",
        "\n",
        "def record(episode,\n",
        "           episode_reward,\n",
        "           worker_idx,\n",
        "           global_ep_reward,\n",
        "           result_queue,\n",
        "           total_loss,\n",
        "           num_steps):\n",
        "    \"\"\"Helper function to store scores and print statistics.\n",
        "\n",
        "    Arguments:\n",
        "      episode: Current episode\n",
        "      episode_reward: Reward accumulated over the current episode\n",
        "      # ?\n",
        "      worker_idx: Which thread (worker)\n",
        "      # ?\n",
        "      global_ep_reward: The moving average of the global reward\n",
        "      result_queue: Queue storing the moving average of the scores\n",
        "      total_loss: The total loss accumulated over the current episode\n",
        "      num_steps: The number of steps the episode took to complete\n",
        "    \"\"\"\n",
        "    # ?\n",
        "    if global_ep_reward == 0:\n",
        "        global_ep_reward = episode_reward\n",
        "    else:\n",
        "        # what is this equation?\n",
        "        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
        "    # https://realpython.com/python-f-strings/\n",
        "    print(\n",
        "        f\"Episode: {episode} | \"\n",
        "        f\"Moving Average Reward: {int(global_ep_reward)} | \"\n",
        "        f\"Episode Reward: {int(episode_reward)} | \"\n",
        "        f\"Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | \"\n",
        "        f\"Steps: {num_steps} | \"\n",
        "        f\"Worker: {worker_idx}\"\n",
        "    )\n",
        "    result_queue.put(global_ep_reward)\n",
        "    return global_ep_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ZjuHpVNTpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## random agent\n",
        "\n",
        "class RandomAgent:\n",
        "    \"\"\"Agent playing randomly\n",
        "    Arguments:\n",
        "      env_name: Name of the environment\n",
        "      max_eps: Maximum number of episodes\n",
        "    \"\"\"\n",
        "    def __init__(self, env_name, max_eps):\n",
        "        self.env = gym.make(env_name)\n",
        "        self.max_episodes = max_eps\n",
        "        self.global_moving_average_reward = 0\n",
        "        # ?\n",
        "        self.res_queue = Queue()\n",
        "\n",
        "    def run(self):\n",
        "        # initialize\n",
        "        reward_avg = 0\n",
        "        for episode in range(self.max_episodes):\n",
        "            # initialize\n",
        "            done = False\n",
        "            self.env.reset()\n",
        "            reward_sum = 0.0\n",
        "            steps = 0\n",
        "            # initially, not False = True\n",
        "            while not done:\n",
        "                # sample actions randomly\n",
        "                # random agent does not care so having underscores\n",
        "                _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
        "                steps += 1\n",
        "                # reward_sum is sum of rewards in each episode\n",
        "                reward_sum += reward\n",
        "                # record statistics\n",
        "                # ?\n",
        "            self.global_moving_average_reward = record(episode,\n",
        "                                                       reward_sum,\n",
        "                                                       0, \n",
        "                                                       self.global_moving_average_reward,\n",
        "                                                       self.res_queue,\n",
        "                                                       0,\n",
        "                                                       steps)\n",
        "            reward_avg += reward_sum\n",
        "        final_avg = reward_avg / float(self.max_episodes)\n",
        "        print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\n",
        "        return final_avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INxBwcAqQ3xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## run random agent\n",
        "\n",
        "# agent = RandomAgent(ENV, 4000)\n",
        "# final_avg = agent.run()\n",
        "# print('final_avg', final_avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxS92GYhgg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## master agent\n",
        "\n",
        "class MasterAgent():\n",
        "    \"\"\"This agent instantiates the global network that\n",
        "    each worker agent will update as well as the optimizer that\n",
        "    we will use to update it.\n",
        "    This master agent holds a shared optimizer that\n",
        "    updates its global network.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.game_name = ENV\n",
        "        save_dir = args.save_dir\n",
        "        self.save_dir = save_dir\n",
        "        # it will automatically make directory\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        env = gym.make(self.game_name)\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        # args.lr? use_locking = True?\n",
        "        # self.opt = tf.train.AdamOptimizer(args.lr, use_locking = True)\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(args.lr, use_locking = True)\n",
        "        print(self.state_size, self.action_size)\n",
        "\n",
        "        # global network\n",
        "        self.global_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "        # np.random.random returns [0.0, 1.0)\n",
        "        self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), \n",
        "                                               dtype = tf.float32))\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Instantiate and start each of the agents\n",
        "        \"\"\"\n",
        "        if args.algorithm == 'random':\n",
        "            random_agent = RandomAgent(self.game_name, args.max_eps)\n",
        "            random_agent.run()\n",
        "            return\n",
        "\n",
        "        # what is this for?\n",
        "        res_queue = Queue()\n",
        "\n",
        "        # ?\n",
        "        workers = [Worker(self.state_size,\n",
        "                          self.action_size,\n",
        "                          self.global_model,\n",
        "                          self.opt,\n",
        "                          res_queue,\n",
        "                          i,\n",
        "                          game_name = self.game_name,\n",
        "                          save_dir = self.save_dir) for i in range(multiprocessing.cpu_count())]\n",
        "\n",
        "        for i, worker in enumerate(workers):\n",
        "            print(\"Starting worker {}\".format(i))\n",
        "            worker.start()\n",
        "\n",
        "        # record episode reward to plot\n",
        "        moving_average_rewards = []\n",
        "        while True:\n",
        "            reward = res_queue.get()\n",
        "            if reward is not None:\n",
        "                moving_average_rewards.append(reward)\n",
        "            else:\n",
        "                break\n",
        "        [w.join() for w in workers]\n",
        "\n",
        "        plt.plot(moving_average_rewards)\n",
        "        plt.ylabel('Moving average ep reward')\n",
        "        plt.xlabel('Step')\n",
        "        plt.savefig(os.path.join(self.save_dir,\n",
        "                                 '{} Moving Average.png'.format(self.game_name)))\n",
        "        plt.show()\n",
        "\n",
        "    def play(self):\n",
        "        env = gym.make(self.game_name).unwrapped\n",
        "        state = env.reset()\n",
        "        model = self.global_model\n",
        "        model_path = os.path.join(self.save_dir,\n",
        "                                  'model_{}.h5'.format(self.game_name))\n",
        "        # model_path = os.path.join(self.save_dir,\n",
        "        #                           'model_{}.h5'.format('CartPole-v0'))\n",
        "        print('Loading model from: {}'.format(model_path))\n",
        "        model.load_weights(model_path)\n",
        "        done = False\n",
        "        step_counter = 0\n",
        "        reward_sum = 0\n",
        "\n",
        "        try:\n",
        "            while not done:\n",
        "                # env.render seems to not work in Google Colab\n",
        "                # env.render(mode = 'rgb_array')\n",
        "                policy, value = model(tf.convert_to_tensor(state[None, :],\n",
        "                                                           dtype = tf.float32))\n",
        "                policy = tf.nn.softmax(policy)\n",
        "                action = np.argmax(policy)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                reward_sum += reward\n",
        "                print(\"{}. Reward: {}, action: {}\".format(step_counter, reward_sum, action))\n",
        "                step_counter += 1\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Received Keyboard Interrupt. Shutting down.\")\n",
        "        finally:\n",
        "            env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfTbv37gM9Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## memory class\n",
        "\n",
        "class Memory:\n",
        "    \"\"\"Keep track of actions, rewards, states\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def store(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsdD3yUzZRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## worker agent\n",
        "\n",
        "class Worker(threading.Thread):\n",
        "    \"\"\"Worker agent\n",
        "    \"\"\"\n",
        "    # Set up global variables across different threads\n",
        "    global_episode = 0\n",
        "    # Movin average reward\n",
        "    global_moving_average_reward = 0\n",
        "    best_score = 0\n",
        "    save_lock = threading.Lock()\n",
        "\n",
        "    def __init__(self,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 global_model,\n",
        "                 opt,\n",
        "                 result_queue,\n",
        "                 idx,\n",
        "                 game_name = ENV,\n",
        "                 save_dir = '/tmp'):\n",
        "        super(Worker, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.result_queue = result_queue\n",
        "        self.global_model = global_model\n",
        "        self.opt = opt\n",
        "        self.local_model = ActorCriticModel(self.state_size,\n",
        "                                            self.action_size)\n",
        "        self.worker_idx = idx\n",
        "        self.game_name = game_name\n",
        "        self.env = gym.make(self.game_name).unwrapped\n",
        "        self.save_dir = save_dir\n",
        "        self.ep_loss = 0.0\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"The agent acts according to policy function as the actor,\n",
        "        while the action is judged by the critic, which is value function\n",
        "        1. get policy (action probability distribution) from current frame\n",
        "        2. Step by action chosen from the policy\n",
        "        3. if the agent has taken a certain number of steps (args.update_freq),\n",
        "           or the agent has reached a terminal state,\n",
        "           then update the global model with gradients computed from local model\n",
        "        4. repeat.\n",
        "        \"\"\"\n",
        "        total_step = 1\n",
        "        mem = Memory()\n",
        "        while Worker.global_episode < args.max_eps:\n",
        "            current_state = self.env.reset()\n",
        "            mem.clear()\n",
        "            ep_reward = 0.\n",
        "            ep_steps = 0\n",
        "            self.ep_loss = 0\n",
        "\n",
        "            time_count = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                logits, _ = self.local_model(\n",
        "                    tf.convert_to_tensor(current_state[None, :],\n",
        "                                         dtype = tf.float32))\n",
        "                probs = tf.nn.softmax(logits)\n",
        "                action = np.random.choice(self.action_size, p = probs.numpy()[0])\n",
        "                new_state, reward, done, _ = self.env.step(action)\n",
        "                # ?\n",
        "                if done:\n",
        "                    reward = -1\n",
        "                ep_reward += reward\n",
        "                mem.store(current_state, action, reward)\n",
        "\n",
        "                if time_count == args.update_freq or done:\n",
        "                    # Calculate gradient with respect to local model.\n",
        "                    # Do so by tracking the variables involved in computing the loss\n",
        "                    # by using tf.GraddientTape\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        total_loss = self.compute_loss(done,\n",
        "                                                       new_state,\n",
        "                                                       mem,\n",
        "                                                       args.gamma)\n",
        "                    self.ep_loss += total_loss\n",
        "                    # Calculate local gradients\n",
        "                    grads = tape.gradient(total_loss, \n",
        "                                          self.local_model.trainable_weights)\n",
        "                    # Push local gradients to global model\n",
        "                    self.opt.apply_gradients(zip(grads,\n",
        "                                                 self.global_model.trainable_weights))\n",
        "                    # Update local model with new weights\n",
        "                    self.local_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "                    mem.clear()\n",
        "                    time_count = 0\n",
        "\n",
        "                    # done and print information\n",
        "                    if done:\n",
        "                        # use helper function\n",
        "                        Worker.global_moving_average_reward = record(Worker.global_episode,\n",
        "                                                                     ep_reward, \n",
        "                                                                     self.worker_idx,\n",
        "                                                                     Worker.global_moving_average_reward,\n",
        "                                                                     self.result_queue,\n",
        "                                                                     self.ep_loss,\n",
        "                                                                     ep_steps)\n",
        "                        # use a lock to save model and to print to prevent data races.\n",
        "                        if ep_reward > Worker.best_score:\n",
        "                            with Worker.save_lock:\n",
        "                                print(\"Saving best model to {}, episode score: {}\".format(self.save_dir, ep_reward))\n",
        "                                self.global_model.save_weights(os.path.join(self.save_dir,\n",
        "                                                                            'model_{}.h5'.format(self.game_name)))\n",
        "                                Worker.best_score = ep_reward\n",
        "                        Worker.global_episode += 1\n",
        "                ep_steps += 1\n",
        "                \n",
        "                time_count += 1\n",
        "                current_state = new_state\n",
        "                total_step += 1\n",
        "        self.result_queue.put(None)\n",
        "\n",
        "    def compute_loss(self,\n",
        "                     done,\n",
        "                     new_state,\n",
        "                     memory,\n",
        "                     gamma = 0.99):\n",
        "        # terminal\n",
        "        if done:\n",
        "            reward_sum = 0.\n",
        "        else:\n",
        "            reward_sum = self.local_model(tf.convert_to_tensor(new_state[None, :],\n",
        "                                                               dtype = tf.float32))[-1].numpy()[0]\n",
        "        # get discounted rewards\n",
        "        discounted_rewards = []\n",
        "        # reverse buffer r?\n",
        "        for reward in memory.rewards[::-1]:\n",
        "            reward_sum = reward + gamma * reward_sum\n",
        "            discounted_rewards.append(reward_sum)\n",
        "        discounted_rewards.reverse()\n",
        "\n",
        "        logits, values = self.local_model(tf.convert_to_tensor(np.vstack(memory.states),\n",
        "                                                               dtype = tf.float32))\n",
        "\n",
        "        # get out advantages\n",
        "        advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None], dtype = tf.float32) - values\n",
        "\n",
        "        # value loss\n",
        "        value_loss = advantage ** 2\n",
        "\n",
        "        # calculate policy loss\n",
        "        policy = tf.nn.softmax(logits)\n",
        "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels = policy, logits = logits)\n",
        "\n",
        "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = memory.actions, logits = logits)\n",
        "\n",
        "        # what is 1e-20?\n",
        "        # entropy = tf.reduce_sum(policy * tf.log(policy + 1e-20), axis = 1)\n",
        "        # actions_one_hot = tf.one_hot(\n",
        "        #     memory.actions,\n",
        "        #     self.action_size,\n",
        "        #     dtype = tf.float32\n",
        "        # )\n",
        "        # policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        #     labels = actions_one_hot,\n",
        "        #     logits = logits\n",
        "        # )\n",
        "\n",
        "        policy_loss *= tf.stop_gradient(advantage)\n",
        "        policy_loss -= 0.01 * entropy\n",
        "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXy1XkbvVXHt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "521e7378-9acd-42dd-af11-834d5b5bd8c1"
      },
      "source": [
        "## training\n",
        "\n",
        "args = Argument()\n",
        "master = MasterAgent()\n",
        "master.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 2\n",
            "Starting worker 0\n",
            "Starting worker 1\n",
            "Episode: 0 | Moving Average Reward: 15 | Episode Reward: 15 | Loss: 2.409 | Steps: 16 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 15.0\n",
            "Episode: 1 | Moving Average Reward: 15 | Episode Reward: 26 | Loss: 2.963 | Steps: 27 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 26.0\n",
            "Episode: 2 | Moving Average Reward: 15 | Episode Reward: 29 | Loss: 2.889 | Steps: 30 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 29.0\n",
            "Episode: 3 | Moving Average Reward: 15 | Episode Reward: 10 | Loss: 1.642 | Steps: 11 | Worker: 0\n",
            "Episode: 4 | Moving Average Reward: 15 | Episode Reward: 12 | Loss: 1.95 | Steps: 13 | Worker: 0Episode: 4 | Moving Average Reward: 15 | Episode Reward: 12 | Loss: 1.923 | Steps: 13 | Worker: 1\n",
            "\n",
            "Episode: 6 | Moving Average Reward: 15 | Episode Reward: 18 | Loss: 2.724 | Steps: 19 | Worker: 1\n",
            "Episode: 7 | Moving Average Reward: 15 | Episode Reward: 20 | Loss: 3.569 | Steps: 21 | Worker: 0\n",
            "Episode: 8 | Moving Average Reward: 15 | Episode Reward: 30 | Loss: 2.887 | Steps: 31 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 30.0\n",
            "Episode: 9 | Moving Average Reward: 15 | Episode Reward: 36 | Loss: 3.017 | Steps: 37 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 36.0\n",
            "Episode: 10 | Moving Average Reward: 15 | Episode Reward: 10 | Loss: 1.605 | Steps: 11 | Worker: 1\n",
            "Episode: 11 | Moving Average Reward: 15 | Episode Reward: 12 | Loss: 1.875 | Steps: 13 | Worker: 0\n",
            "Episode: 12 | Moving Average Reward: 15 | Episode Reward: 17 | Loss: 2.545 | Steps: 18 | Worker: 0\n",
            "Episode: 13 | Moving Average Reward: 15 | Episode Reward: 21 | Loss: 3.436 | Steps: 22 | Worker: 1\n",
            "Episode: 14 | Moving Average Reward: 15 | Episode Reward: 15 | Loss: 2.271 | Steps: 16 | Worker: 0\n",
            "Episode: 15 | Moving Average Reward: 15 | Episode Reward: 12 | Loss: 1.862 | Steps: 13 | Worker: 1\n",
            "Episode: 16 | Moving Average Reward: 15 | Episode Reward: 14 | Loss: 2.132 | Steps: 15 | Worker: 1\n",
            "Episode: 17 | Moving Average Reward: 15 | Episode Reward: 34 | Loss: 2.96 | Steps: 35 | Worker: 0\n",
            "Episode: 18 | Moving Average Reward: 15 | Episode Reward: 19 | Loss: 2.795 | Steps: 20 | Worker: 1\n",
            "Episode: 19 | Moving Average Reward: 15 | Episode Reward: 27 | Loss: 2.905 | Steps: 28 | Worker: 0\n",
            "Episode: 20 | Moving Average Reward: 16 | Episode Reward: 38 | Loss: 3.063 | Steps: 39 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 38.0\n",
            "Episode: 21 | Moving Average Reward: 16 | Episode Reward: 12 | Loss: 1.869 | Steps: 13 | Worker: 0\n",
            "Episode: 22 | Moving Average Reward: 15 | Episode Reward: 8 | Loss: 1.219 | Steps: 9 | Worker: 0\n",
            "Episode: 23 | Moving Average Reward: 16 | Episode Reward: 22 | Loss: 3.33 | Steps: 23 | Worker: 1\n",
            "Episode: 24 | Moving Average Reward: 16 | Episode Reward: 16 | Loss: 2.331 | Steps: 17 | Worker: 1\n",
            "Episode: 25 | Moving Average Reward: 16 | Episode Reward: 29 | Loss: 2.808 | Steps: 30 | Worker: 0\n",
            "Episode: 26 | Moving Average Reward: 16 | Episode Reward: 36 | Loss: 2.982 | Steps: 37 | Worker: 1\n",
            "Episode: 27 | Moving Average Reward: 16 | Episode Reward: 20 | Loss: 3.566 | Steps: 21 | Worker: 0\n",
            "Episode: 28 | Moving Average Reward: 16 | Episode Reward: 23 | Loss: 3.16 | Steps: 24 | Worker: 1\n",
            "Episode: 29 | Moving Average Reward: 16 | Episode Reward: 11 | Loss: 1.647 | Steps: 12 | Worker: 0\n",
            "Episode: 30 | Moving Average Reward: 16 | Episode Reward: 18 | Loss: 2.59 | Steps: 19 | Worker: 0\n",
            "Episode: 31 | Moving Average Reward: 16 | Episode Reward: 28 | Loss: 2.864 | Steps: 29 | Worker: 1\n",
            "Episode: 32 | Moving Average Reward: 16 | Episode Reward: 15 | Loss: 2.154 | Steps: 16 | Worker: 1\n",
            "Episode: 33 | Moving Average Reward: 16 | Episode Reward: 9 | Loss: 1.24 | Steps: 10 | Worker: 1\n",
            "Episode: 34 | Moving Average Reward: 16 | Episode Reward: 42 | Loss: 3.413 | Steps: 43 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 42.0\n",
            "Episode: 35 | Moving Average Reward: 16 | Episode Reward: 14 | Loss: 1.993 | Steps: 15 | Worker: 1\n",
            "Episode: 36 | Moving Average Reward: 16 | Episode Reward: 23 | Loss: 3.271 | Steps: 24 | Worker: 0\n",
            "Episode: 37 | Moving Average Reward: 16 | Episode Reward: 23 | Loss: 3.177 | Steps: 24 | Worker: 1\n",
            "Episode: 38 | Moving Average Reward: 16 | Episode Reward: 13 | Loss: 1.873 | Steps: 14 | Worker: 0\n",
            "Episode: 39 | Moving Average Reward: 16 | Episode Reward: 18 | Loss: 2.499 | Steps: 19 | Worker: 1\n",
            "Episode: 40 | Moving Average Reward: 16 | Episode Reward: 14 | Loss: 1.979 | Steps: 15 | Worker: 1Episode: 40 | Moving Average Reward: 16 | Episode Reward: 28 | Loss: 2.814 | Steps: 29 | Worker: 0\n",
            "\n",
            "Episode: 42 | Moving Average Reward: 16 | Episode Reward: 11 | Loss: 1.513 | Steps: 12 | Worker: 0\n",
            "Episode: 43 | Moving Average Reward: 16 | Episode Reward: 22 | Loss: 3.151 | Steps: 23 | Worker: 1\n",
            "Episode: 44 | Moving Average Reward: 16 | Episode Reward: 20 | Loss: 3.631 | Steps: 21 | Worker: 0\n",
            "Episode: 45 | Moving Average Reward: 16 | Episode Reward: 15 | Loss: 2.093 | Steps: 16 | Worker: 1\n",
            "Episode: 46 | Moving Average Reward: 16 | Episode Reward: 15 | Loss: 2.12 | Steps: 16 | Worker: 0\n",
            "Episode: 47 | Moving Average Reward: 16 | Episode Reward: 24 | Loss: 3.046 | Steps: 25 | Worker: 1\n",
            "Episode: 48 | Moving Average Reward: 17 | Episode Reward: 25 | Loss: 3.002 | Steps: 26 | Worker: 0\n",
            "Episode: 49 | Moving Average Reward: 17 | Episode Reward: 26 | Loss: 2.902 | Steps: 27 | Worker: 0\n",
            "Episode: 50 | Moving Average Reward: 17 | Episode Reward: 42 | Loss: 3.35 | Steps: 43 | Worker: 1\n",
            "Episode: 51 | Moving Average Reward: 17 | Episode Reward: 19 | Loss: 2.54 | Steps: 20 | Worker: 0Episode: 51 | Moving Average Reward: 17 | Episode Reward: 17 | Loss: 2.335 | Steps: 18 | Worker: 1\n",
            "\n",
            "Episode: 53 | Moving Average Reward: 17 | Episode Reward: 11 | Loss: 1.45 | Steps: 12 | Worker: 0\n",
            "Episode: 54 | Moving Average Reward: 17 | Episode Reward: 34 | Loss: 2.83 | Steps: 35 | Worker: 1\n",
            "Episode: 55 | Moving Average Reward: 17 | Episode Reward: 12 | Loss: 1.538 | Steps: 13 | Worker: 1\n",
            "Episode: 55 | Moving Average Reward: 17 | Episode Reward: 23 | Loss: 3.209 | Steps: 24 | Worker: 0\n",
            "Episode: 57 | Moving Average Reward: 17 | Episode Reward: 16 | Loss: 2.134 | Steps: 17 | Worker: 0\n",
            "Episode: 58 | Moving Average Reward: 17 | Episode Reward: 17 | Loss: 2.232 | Steps: 18 | Worker: 1\n",
            "Episode: 59 | Moving Average Reward: 17 | Episode Reward: 10 | Loss: 1.295 | Steps: 11 | Worker: 0\n",
            "Episode: 60 | Moving Average Reward: 17 | Episode Reward: 20 | Loss: 3.612 | Steps: 21 | Worker: 1\n",
            "Episode: 61 | Moving Average Reward: 17 | Episode Reward: 15 | Loss: 1.982 | Steps: 16 | Worker: 1\n",
            "Episode: 62 | Moving Average Reward: 17 | Episode Reward: 27 | Loss: 2.917 | Steps: 28 | Worker: 0\n",
            "Episode: 63 | Moving Average Reward: 17 | Episode Reward: 30 | Loss: 2.676 | Steps: 31 | Worker: 1Episode: 63 | Moving Average Reward: 17 | Episode Reward: 22 | Loss: 3.17 | Steps: 23 | Worker: 0\n",
            "\n",
            "Episode: 65 | Moving Average Reward: 17 | Episode Reward: 18 | Loss: 2.357 | Steps: 19 | Worker: 0\n",
            "Episode: 66 | Moving Average Reward: 17 | Episode Reward: 13 | Loss: 1.669 | Steps: 14 | Worker: 1\n",
            "Episode: 67 | Moving Average Reward: 17 | Episode Reward: 9 | Loss: 1.073 | Steps: 10 | Worker: 1Episode: 67 | Moving Average Reward: 17 | Episode Reward: 11 | Loss: 1.335 | Steps: 12 | Worker: 0\n",
            "\n",
            "Episode: 69 | Moving Average Reward: 17 | Episode Reward: 32 | Loss: 2.692 | Steps: 33 | Worker: 0\n",
            "Episode: 70 | Moving Average Reward: 17 | Episode Reward: 21 | Loss: 3.78 | Steps: 22 | Worker: 1\n",
            "Episode: 71 | Moving Average Reward: 17 | Episode Reward: 12 | Loss: 1.316 | Steps: 13 | Worker: 1\n",
            "Episode: 72 | Moving Average Reward: 17 | Episode Reward: 10 | Loss: 1.262 | Steps: 11 | Worker: 1\n",
            "Episode: 73 | Moving Average Reward: 17 | Episode Reward: 21 | Loss: 3.238 | Steps: 22 | Worker: 0\n",
            "Episode: 74 | Moving Average Reward: 17 | Episode Reward: 13 | Loss: 1.523 | Steps: 14 | Worker: 1\n",
            "Episode: 75 | Moving Average Reward: 17 | Episode Reward: 14 | Loss: 1.733 | Steps: 15 | Worker: 1\n",
            "Episode: 76 | Moving Average Reward: 17 | Episode Reward: 25 | Loss: 2.823 | Steps: 26 | Worker: 0\n",
            "Episode: 77 | Moving Average Reward: 17 | Episode Reward: 10 | Loss: 1.187 | Steps: 11 | Worker: 0\n",
            "Episode: 78 | Moving Average Reward: 17 | Episode Reward: 9 | Loss: 0.999 | Steps: 10 | Worker: 0\n",
            "Episode: 79 | Moving Average Reward: 17 | Episode Reward: 13 | Loss: 1.506 | Steps: 14 | Worker: 0\n",
            "Episode: 80 | Moving Average Reward: 17 | Episode Reward: 10 | Loss: 1.093 | Steps: 11 | Worker: 0\n",
            "Episode: 81 | Moving Average Reward: 17 | Episode Reward: 31 | Loss: 2.614 | Steps: 32 | Worker: 0\n",
            "Episode: 82 | Moving Average Reward: 18 | Episode Reward: 138 | Loss: 3.219 | Steps: 139 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 138.0\n",
            "Episode: 83 | Moving Average Reward: 18 | Episode Reward: 35 | Loss: 2.563 | Steps: 36 | Worker: 0\n",
            "Episode: 84 | Moving Average Reward: 18 | Episode Reward: 15 | Loss: 1.688 | Steps: 16 | Worker: 0\n",
            "Episode: 85 | Moving Average Reward: 18 | Episode Reward: 36 | Loss: 2.841 | Steps: 37 | Worker: 1\n",
            "Episode: 86 | Moving Average Reward: 18 | Episode Reward: 29 | Loss: 2.645 | Steps: 30 | Worker: 0\n",
            "Episode: 87 | Moving Average Reward: 19 | Episode Reward: 33 | Loss: 2.594 | Steps: 34 | Worker: 1\n",
            "Episode: 88 | Moving Average Reward: 18 | Episode Reward: 12 | Loss: 1.256 | Steps: 13 | Worker: 1\n",
            "Episode: 89 | Moving Average Reward: 19 | Episode Reward: 20 | Loss: 3.632 | Steps: 21 | Worker: 1\n",
            "Episode: 90 | Moving Average Reward: 19 | Episode Reward: 52 | Loss: 2.901 | Steps: 53 | Worker: 0\n",
            "Episode: 91 | Moving Average Reward: 19 | Episode Reward: 18 | Loss: 2.048 | Steps: 19 | Worker: 1\n",
            "Episode: 92 | Moving Average Reward: 19 | Episode Reward: 43 | Loss: 3.145 | Steps: 44 | Worker: 0\n",
            "Episode: 93 | Moving Average Reward: 19 | Episode Reward: 35 | Loss: 2.534 | Steps: 36 | Worker: 1\n",
            "Episode: 94 | Moving Average Reward: 19 | Episode Reward: 28 | Loss: 2.651 | Steps: 29 | Worker: 1\n",
            "Episode: 95 | Moving Average Reward: 19 | Episode Reward: 29 | Loss: 2.595 | Steps: 30 | Worker: 0\n",
            "Episode: 96 | Moving Average Reward: 19 | Episode Reward: 11 | Loss: 1.099 | Steps: 12 | Worker: 0\n",
            "Episode: 97 | Moving Average Reward: 19 | Episode Reward: 25 | Loss: 2.867 | Steps: 26 | Worker: 0\n",
            "Episode: 98 | Moving Average Reward: 19 | Episode Reward: 27 | Loss: 2.637 | Steps: 28 | Worker: 0\n",
            "Episode: 99 | Moving Average Reward: 19 | Episode Reward: 14 | Loss: 1.37 | Steps: 15 | Worker: 0\n",
            "Episode: 100 | Moving Average Reward: 20 | Episode Reward: 122 | Loss: 3.242 | Steps: 123 | Worker: 1\n",
            "Episode: 101 | Moving Average Reward: 20 | Episode Reward: 21 | Loss: 3.714 | Steps: 22 | Worker: 0\n",
            "Episode: 102 | Moving Average Reward: 20 | Episode Reward: 18 | Loss: 1.834 | Steps: 19 | Worker: 1\n",
            "Episode: 103 | Moving Average Reward: 20 | Episode Reward: 10 | Loss: 0.854 | Steps: 11 | Worker: 0\n",
            "Episode: 104 | Moving Average Reward: 20 | Episode Reward: 10 | Loss: 0.88 | Steps: 11 | Worker: 0\n",
            "Episode: 105 | Moving Average Reward: 20 | Episode Reward: 25 | Loss: 2.807 | Steps: 26 | Worker: 1\n",
            "Episode: 106 | Moving Average Reward: 20 | Episode Reward: 28 | Loss: 2.549 | Steps: 29 | Worker: 0\n",
            "Episode: 107 | Moving Average Reward: 20 | Episode Reward: 12 | Loss: 0.931 | Steps: 13 | Worker: 0\n",
            "Episode: 108 | Moving Average Reward: 21 | Episode Reward: 61 | Loss: 3.309 | Steps: 62 | Worker: 1\n",
            "Episode: 109 | Moving Average Reward: 21 | Episode Reward: 25 | Loss: 3.144 | Steps: 26 | Worker: 0\n",
            "Episode: 110 | Moving Average Reward: 21 | Episode Reward: 30 | Loss: 2.469 | Steps: 31 | Worker: 0\n",
            "Episode: 111 | Moving Average Reward: 21 | Episode Reward: 53 | Loss: 2.718 | Steps: 54 | Worker: 1\n",
            "Episode: 112 | Moving Average Reward: 21 | Episode Reward: 29 | Loss: 2.55 | Steps: 30 | Worker: 0\n",
            "Episode: 113 | Moving Average Reward: 21 | Episode Reward: 40 | Loss: 3.699 | Steps: 41 | Worker: 1\n",
            "Episode: 114 | Moving Average Reward: 21 | Episode Reward: 34 | Loss: 2.463 | Steps: 35 | Worker: 0\n",
            "Episode: 115 | Moving Average Reward: 21 | Episode Reward: 9 | Loss: 0.762 | Steps: 10 | Worker: 1\n",
            "Episode: 116 | Moving Average Reward: 21 | Episode Reward: 33 | Loss: 2.436 | Steps: 34 | Worker: 0\n",
            "Episode: 117 | Moving Average Reward: 22 | Episode Reward: 45 | Loss: 2.989 | Steps: 46 | Worker: 1\n",
            "Episode: 118 | Moving Average Reward: 22 | Episode Reward: 57 | Loss: 2.726 | Steps: 58 | Worker: 0\n",
            "Episode: 119 | Moving Average Reward: 22 | Episode Reward: 53 | Loss: 2.777 | Steps: 54 | Worker: 1\n",
            "Episode: 120 | Moving Average Reward: 23 | Episode Reward: 53 | Loss: 2.659 | Steps: 54 | Worker: 0\n",
            "Episode: 121 | Moving Average Reward: 23 | Episode Reward: 17 | Loss: 1.369 | Steps: 18 | Worker: 1\n",
            "Episode: 122 | Moving Average Reward: 22 | Episode Reward: 18 | Loss: 1.548 | Steps: 19 | Worker: 1\n",
            "Episode: 123 | Moving Average Reward: 22 | Episode Reward: 25 | Loss: 2.75 | Steps: 26 | Worker: 0\n",
            "Episode: 124 | Moving Average Reward: 23 | Episode Reward: 29 | Loss: 2.453 | Steps: 30 | Worker: 1\n",
            "Episode: 125 | Moving Average Reward: 23 | Episode Reward: 21 | Loss: 3.448 | Steps: 22 | Worker: 0\n",
            "Episode: 126 | Moving Average Reward: 23 | Episode Reward: 35 | Loss: 2.335 | Steps: 36 | Worker: 0\n",
            "Episode: 127 | Moving Average Reward: 23 | Episode Reward: 41 | Loss: 3.49 | Steps: 42 | Worker: 1\n",
            "Episode: 128 | Moving Average Reward: 23 | Episode Reward: 17 | Loss: 1.309 | Steps: 18 | Worker: 1\n",
            "Episode: 129 | Moving Average Reward: 23 | Episode Reward: 53 | Loss: 2.849 | Steps: 54 | Worker: 0\n",
            "Episode: 130 | Moving Average Reward: 23 | Episode Reward: 48 | Loss: 2.765 | Steps: 49 | Worker: 1\n",
            "Episode: 131 | Moving Average Reward: 23 | Episode Reward: 38 | Loss: 2.344 | Steps: 39 | Worker: 0\n",
            "Episode: 132 | Moving Average Reward: 23 | Episode Reward: 18 | Loss: 1.316 | Steps: 19 | Worker: 1\n",
            "Episode: 133 | Moving Average Reward: 23 | Episode Reward: 34 | Loss: 2.449 | Steps: 35 | Worker: 0\n",
            "Episode: 134 | Moving Average Reward: 24 | Episode Reward: 40 | Loss: 3.503 | Steps: 41 | Worker: 0\n",
            "Episode: 135 | Moving Average Reward: 24 | Episode Reward: 73 | Loss: 2.742 | Steps: 74 | Worker: 1\n",
            "Episode: 136 | Moving Average Reward: 24 | Episode Reward: 32 | Loss: 2.238 | Steps: 33 | Worker: 0\n",
            "Episode: 137 | Moving Average Reward: 24 | Episode Reward: 30 | Loss: 2.312 | Steps: 31 | Worker: 0\n",
            "Episode: 138 | Moving Average Reward: 24 | Episode Reward: 29 | Loss: 2.419 | Steps: 30 | Worker: 0\n",
            "Episode: 139 | Moving Average Reward: 24 | Episode Reward: 17 | Loss: 1.109 | Steps: 18 | Worker: 0\n",
            "Episode: 139 | Moving Average Reward: 25 | Episode Reward: 96 | Loss: 2.758 | Steps: 97 | Worker: 1\n",
            "Episode: 141 | Moving Average Reward: 24 | Episode Reward: 35 | Loss: 2.196 | Steps: 36 | Worker: 0\n",
            "Episode: 142 | Moving Average Reward: 25 | Episode Reward: 60 | Loss: 3.536 | Steps: 61 | Worker: 1\n",
            "Episode: 143 | Moving Average Reward: 25 | Episode Reward: 51 | Loss: 2.59 | Steps: 52 | Worker: 0\n",
            "Episode: 144 | Moving Average Reward: 25 | Episode Reward: 44 | Loss: 2.988 | Steps: 45 | Worker: 1\n",
            "Episode: 145 | Moving Average Reward: 25 | Episode Reward: 23 | Loss: 3.052 | Steps: 24 | Worker: 0\n",
            "Episode: 146 | Moving Average Reward: 25 | Episode Reward: 28 | Loss: 2.537 | Steps: 29 | Worker: 1\n",
            "Episode: 147 | Moving Average Reward: 25 | Episode Reward: 32 | Loss: 2.19 | Steps: 33 | Worker: 0\n",
            "Episode: 148 | Moving Average Reward: 25 | Episode Reward: 24 | Loss: 3.377 | Steps: 25 | Worker: 1\n",
            "Episode: 149 | Moving Average Reward: 26 | Episode Reward: 63 | Loss: 3.076 | Steps: 64 | Worker: 0\n",
            "Episode: 150 | Moving Average Reward: 26 | Episode Reward: 112 | Loss: 2.957 | Steps: 113 | Worker: 1\n",
            "Episode: 151 | Moving Average Reward: 26 | Episode Reward: 16 | Loss: 0.761 | Steps: 17 | Worker: 1\n",
            "Episode: 152 | Moving Average Reward: 27 | Episode Reward: 94 | Loss: 2.694 | Steps: 95 | Worker: 0\n",
            "Episode: 153 | Moving Average Reward: 27 | Episode Reward: 34 | Loss: 2.188 | Steps: 35 | Worker: 1\n",
            "Episode: 154 | Moving Average Reward: 27 | Episode Reward: 16 | Loss: 0.765 | Steps: 17 | Worker: 1\n",
            "Episode: 155 | Moving Average Reward: 27 | Episode Reward: 43 | Loss: 3.145 | Steps: 44 | Worker: 1\n",
            "Episode: 156 | Moving Average Reward: 28 | Episode Reward: 107 | Loss: 3.006 | Steps: 108 | Worker: 0\n",
            "Episode: 157 | Moving Average Reward: 28 | Episode Reward: 37 | Loss: 2.022 | Steps: 38 | Worker: 1\n",
            "Episode: 158 | Moving Average Reward: 28 | Episode Reward: 18 | Loss: 0.856 | Steps: 19 | Worker: 0\n",
            "Episode: 159 | Moving Average Reward: 28 | Episode Reward: 19 | Loss: 0.908 | Steps: 20 | Worker: 1\n",
            "Episode: 160 | Moving Average Reward: 28 | Episode Reward: 50 | Loss: 2.672 | Steps: 51 | Worker: 0\n",
            "Episode: 161 | Moving Average Reward: 28 | Episode Reward: 61 | Loss: 3.278 | Steps: 62 | Worker: 1\n",
            "Episode: 162 | Moving Average Reward: 28 | Episode Reward: 31 | Loss: 2.106 | Steps: 32 | Worker: 1\n",
            "Episode: 163 | Moving Average Reward: 29 | Episode Reward: 77 | Loss: 2.436 | Steps: 78 | Worker: 0\n",
            "Episode: 164 | Moving Average Reward: 29 | Episode Reward: 20 | Loss: 4.784 | Steps: 21 | Worker: 0\n",
            "Episode: 165 | Moving Average Reward: 29 | Episode Reward: 34 | Loss: 2.125 | Steps: 35 | Worker: 1\n",
            "Episode: 166 | Moving Average Reward: 29 | Episode Reward: 35 | Loss: 2.035 | Steps: 36 | Worker: 0\n",
            "Episode: 167 | Moving Average Reward: 29 | Episode Reward: 89 | Loss: 2.787 | Steps: 90 | Worker: 1\n",
            "Episode: 168 | Moving Average Reward: 30 | Episode Reward: 123 | Loss: 3.242 | Steps: 124 | Worker: 0\n",
            "Episode: 169 | Moving Average Reward: 31 | Episode Reward: 65 | Loss: 3.068 | Steps: 66 | Worker: 1\n",
            "Episode: 170 | Moving Average Reward: 31 | Episode Reward: 41 | Loss: 3.428 | Steps: 42 | Worker: 1\n",
            "Episode: 171 | Moving Average Reward: 31 | Episode Reward: 36 | Loss: 2.166 | Steps: 37 | Worker: 1\n",
            "Episode: 172 | Moving Average Reward: 32 | Episode Reward: 114 | Loss: 2.658 | Steps: 115 | Worker: 0\n",
            "Episode: 173 | Moving Average Reward: 32 | Episode Reward: 57 | Loss: 2.254 | Steps: 58 | Worker: 0\n",
            "Episode: 174 | Moving Average Reward: 32 | Episode Reward: 83 | Loss: 3.459 | Steps: 84 | Worker: 1\n",
            "Episode: 175 | Moving Average Reward: 33 | Episode Reward: 79 | Loss: 2.303 | Steps: 80 | Worker: 0\n",
            "Episode: 176 | Moving Average Reward: 33 | Episode Reward: 75 | Loss: 2.354 | Steps: 76 | Worker: 1\n",
            "Episode: 177 | Moving Average Reward: 33 | Episode Reward: 30 | Loss: 2.348 | Steps: 31 | Worker: 0\n",
            "Episode: 178 | Moving Average Reward: 34 | Episode Reward: 70 | Loss: 2.721 | Steps: 71 | Worker: 1\n",
            "Episode: 179 | Moving Average Reward: 34 | Episode Reward: 62 | Loss: 3.553 | Steps: 63 | Worker: 0\n",
            "Episode: 180 | Moving Average Reward: 34 | Episode Reward: 11 | Loss: 1.252 | Steps: 12 | Worker: 0\n",
            "Episode: 181 | Moving Average Reward: 34 | Episode Reward: 41 | Loss: 4.231 | Steps: 42 | Worker: 1\n",
            "Episode: 182 | Moving Average Reward: 34 | Episode Reward: 66 | Loss: 3.286 | Steps: 67 | Worker: 1\n",
            "Episode: 183 | Moving Average Reward: 35 | Episode Reward: 105 | Loss: 2.936 | Steps: 106 | Worker: 0\n",
            "Episode: 184 | Moving Average Reward: 35 | Episode Reward: 37 | Loss: 2.106 | Steps: 38 | Worker: 1\n",
            "Episode: 185 | Moving Average Reward: 35 | Episode Reward: 15 | Loss: 0.913 | Steps: 16 | Worker: 0\n",
            "Episode: 186 | Moving Average Reward: 34 | Episode Reward: 25 | Loss: 3.987 | Steps: 26 | Worker: 1\n",
            "Episode: 187 | Moving Average Reward: 34 | Episode Reward: 24 | Loss: 4.377 | Steps: 25 | Worker: 1\n",
            "Episode: 188 | Moving Average Reward: 35 | Episode Reward: 103 | Loss: 3.759 | Steps: 104 | Worker: 0\n",
            "Episode: 189 | Moving Average Reward: 35 | Episode Reward: 20 | Loss: 5.761 | Steps: 21 | Worker: 0\n",
            "Episode: 190 | Moving Average Reward: 36 | Episode Reward: 109 | Loss: 2.902 | Steps: 110 | Worker: 1\n",
            "Episode: 191 | Moving Average Reward: 35 | Episode Reward: 23 | Loss: 4.352 | Steps: 24 | Worker: 0\n",
            "Episode: 192 | Moving Average Reward: 36 | Episode Reward: 47 | Loss: 3.067 | Steps: 48 | Worker: 0\n",
            "Episode: 193 | Moving Average Reward: 36 | Episode Reward: 119 | Loss: 2.3 | Steps: 120 | Worker: 1\n",
            "Episode: 194 | Moving Average Reward: 37 | Episode Reward: 103 | Loss: 3.693 | Steps: 104 | Worker: 1\n",
            "Episode: 195 | Moving Average Reward: 38 | Episode Reward: 152 | Loss: 2.53 | Steps: 153 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 152.0\n",
            "Episode: 196 | Moving Average Reward: 38 | Episode Reward: 40 | Loss: 4.402 | Steps: 41 | Worker: 1\n",
            "Episode: 197 | Moving Average Reward: 38 | Episode Reward: 25 | Loss: 4.147 | Steps: 26 | Worker: 0\n",
            "Episode: 198 | Moving Average Reward: 39 | Episode Reward: 132 | Loss: 4.21 | Steps: 133 | Worker: 0\n",
            "Episode: 199 | Moving Average Reward: 40 | Episode Reward: 174 | Loss: 3.785 | Steps: 175 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 174.0\n",
            "Episode: 200 | Moving Average Reward: 40 | Episode Reward: 29 | Loss: 3.867 | Steps: 30 | Worker: 0\n",
            "Episode: 201 | Moving Average Reward: 40 | Episode Reward: 54 | Loss: 2.172 | Steps: 55 | Worker: 1\n",
            "Episode: 202 | Moving Average Reward: 40 | Episode Reward: 23 | Loss: 6.516 | Steps: 24 | Worker: 1\n",
            "Episode: 203 | Moving Average Reward: 41 | Episode Reward: 124 | Loss: 3.922 | Steps: 125 | Worker: 0\n",
            "Episode: 204 | Moving Average Reward: 41 | Episode Reward: 55 | Loss: 3.148 | Steps: 56 | Worker: 1\n",
            "Episode: 205 | Moving Average Reward: 42 | Episode Reward: 89 | Loss: 2.921 | Steps: 90 | Worker: 0\n",
            "Episode: 206 | Moving Average Reward: 42 | Episode Reward: 72 | Loss: 2.407 | Steps: 73 | Worker: 1\n",
            "Episode: 207 | Moving Average Reward: 42 | Episode Reward: 30 | Loss: 3.215 | Steps: 31 | Worker: 0\n",
            "Episode: 208 | Moving Average Reward: 42 | Episode Reward: 80 | Loss: 3.525 | Steps: 81 | Worker: 1\n",
            "Episode: 209 | Moving Average Reward: 43 | Episode Reward: 97 | Loss: 2.469 | Steps: 98 | Worker: 1\n",
            "Episode: 210 | Moving Average Reward: 43 | Episode Reward: 65 | Loss: 3.182 | Steps: 66 | Worker: 1\n",
            "Episode: 211 | Moving Average Reward: 46 | Episode Reward: 337 | Loss: 2.383 | Steps: 338 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 337.0\n",
            "Episode: 212 | Moving Average Reward: 47 | Episode Reward: 124 | Loss: 2.728 | Steps: 125 | Worker: 1\n",
            "Episode: 213 | Moving Average Reward: 47 | Episode Reward: 79 | Loss: 2.304 | Steps: 80 | Worker: 0\n",
            "Episode: 214 | Moving Average Reward: 47 | Episode Reward: 34 | Loss: 2.839 | Steps: 35 | Worker: 0\n",
            "Episode: 215 | Moving Average Reward: 47 | Episode Reward: 105 | Loss: 2.415 | Steps: 106 | Worker: 1\n",
            "Episode: 216 | Moving Average Reward: 48 | Episode Reward: 71 | Loss: 2.85 | Steps: 72 | Worker: 1\n",
            "Episode: 217 | Moving Average Reward: 49 | Episode Reward: 191 | Loss: 2.864 | Steps: 192 | Worker: 0\n",
            "Episode: 218 | Moving Average Reward: 49 | Episode Reward: 39 | Loss: 2.808 | Steps: 40 | Worker: 0\n",
            "Episode: 219 | Moving Average Reward: 50 | Episode Reward: 187 | Loss: 3.426 | Steps: 188 | Worker: 1\n",
            "Episode: 220 | Moving Average Reward: 52 | Episode Reward: 182 | Loss: 2.593 | Steps: 183 | Worker: 0\n",
            "Episode: 221 | Moving Average Reward: 53 | Episode Reward: 185 | Loss: 3.388 | Steps: 186 | Worker: 1\n",
            "Episode: 222 | Moving Average Reward: 54 | Episode Reward: 172 | Loss: 2.115 | Steps: 173 | Worker: 0\n",
            "Episode: 223 | Moving Average Reward: 55 | Episode Reward: 120 | Loss: 3.752 | Steps: 121 | Worker: 1\n",
            "Episode: 224 | Moving Average Reward: 55 | Episode Reward: 63 | Loss: 2.736 | Steps: 64 | Worker: 0\n",
            "Episode: 225 | Moving Average Reward: 56 | Episode Reward: 145 | Loss: 2.376 | Steps: 146 | Worker: 1\n",
            "Episode: 226 | Moving Average Reward: 57 | Episode Reward: 184 | Loss: 2.407 | Steps: 185 | Worker: 0\n",
            "Episode: 227 | Moving Average Reward: 58 | Episode Reward: 160 | Loss: 4.053 | Steps: 161 | Worker: 1\n",
            "Episode: 228 | Moving Average Reward: 58 | Episode Reward: 95 | Loss: 2.503 | Steps: 96 | Worker: 0\n",
            "Episode: 229 | Moving Average Reward: 61 | Episode Reward: 326 | Loss: 2.254 | Steps: 327 | Worker: 1\n",
            "Episode: 230 | Moving Average Reward: 62 | Episode Reward: 106 | Loss: 6.038 | Steps: 107 | Worker: 1\n",
            "Episode: 231 | Moving Average Reward: 66 | Episode Reward: 456 | Loss: 2.318 | Steps: 457 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 456.0\n",
            "Episode: 232 | Moving Average Reward: 66 | Episode Reward: 139 | Loss: 5.635 | Steps: 140 | Worker: 0\n",
            "Episode: 233 | Moving Average Reward: 68 | Episode Reward: 210 | Loss: 2.582 | Steps: 211 | Worker: 1\n",
            "Episode: 234 | Moving Average Reward: 69 | Episode Reward: 196 | Loss: 3.827 | Steps: 197 | Worker: 1\n",
            "Episode: 235 | Moving Average Reward: 71 | Episode Reward: 239 | Loss: 2.095 | Steps: 240 | Worker: 0\n",
            "Episode: 236 | Moving Average Reward: 71 | Episode Reward: 128 | Loss: 3.406 | Steps: 129 | Worker: 1\n",
            "Episode: 237 | Moving Average Reward: 72 | Episode Reward: 155 | Loss: 5.871 | Steps: 156 | Worker: 0\n",
            "Episode: 238 | Moving Average Reward: 72 | Episode Reward: 97 | Loss: 4.49 | Steps: 98 | Worker: 1\n",
            "Episode: 239 | Moving Average Reward: 73 | Episode Reward: 175 | Loss: 4.492 | Steps: 176 | Worker: 0\n",
            "Episode: 240 | Moving Average Reward: 74 | Episode Reward: 185 | Loss: 4.072 | Steps: 186 | Worker: 1\n",
            "Episode: 241 | Moving Average Reward: 75 | Episode Reward: 177 | Loss: 4.78 | Steps: 178 | Worker: 0\n",
            "Episode: 242 | Moving Average Reward: 76 | Episode Reward: 133 | Loss: 5.938 | Steps: 134 | Worker: 1\n",
            "Episode: 243 | Moving Average Reward: 76 | Episode Reward: 105 | Loss: 7.388 | Steps: 106 | Worker: 0\n",
            "Episode: 244 | Moving Average Reward: 77 | Episode Reward: 109 | Loss: 7.966 | Steps: 110 | Worker: 1\n",
            "Episode: 245 | Moving Average Reward: 78 | Episode Reward: 163 | Loss: 5.392 | Steps: 164 | Worker: 0\n",
            "Episode: 246 | Moving Average Reward: 78 | Episode Reward: 167 | Loss: 5.01 | Steps: 168 | Worker: 1\n",
            "Episode: 247 | Moving Average Reward: 80 | Episode Reward: 258 | Loss: 3.101 | Steps: 259 | Worker: 1\n",
            "Episode: 248 | Moving Average Reward: 84 | Episode Reward: 493 | Loss: 2.733 | Steps: 494 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 493.0\n",
            "Episode: 249 | Moving Average Reward: 87 | Episode Reward: 340 | Loss: 4.347 | Steps: 341 | Worker: 1\n",
            "Episode: 250 | Moving Average Reward: 87 | Episode Reward: 137 | Loss: 5.315 | Steps: 138 | Worker: 0\n",
            "Episode: 251 | Moving Average Reward: 87 | Episode Reward: 84 | Loss: 7.814 | Steps: 85 | Worker: 0\n",
            "Episode: 252 | Moving Average Reward: 87 | Episode Reward: 55 | Loss: 8.406 | Steps: 56 | Worker: 0\n",
            "Episode: 253 | Moving Average Reward: 88 | Episode Reward: 182 | Loss: 4.143 | Steps: 183 | Worker: 1\n",
            "Episode: 254 | Moving Average Reward: 90 | Episode Reward: 282 | Loss: 3.513 | Steps: 283 | Worker: 1\n",
            "Episode: 255 | Moving Average Reward: 93 | Episode Reward: 440 | Loss: 2.903 | Steps: 441 | Worker: 0\n",
            "Episode: 256 | Moving Average Reward: 95 | Episode Reward: 243 | Loss: 3.233 | Steps: 244 | Worker: 1\n",
            "Episode: 257 | Moving Average Reward: 96 | Episode Reward: 162 | Loss: 4.613 | Steps: 163 | Worker: 0\n",
            "Episode: 258 | Moving Average Reward: 96 | Episode Reward: 145 | Loss: 5.689 | Steps: 146 | Worker: 1\n",
            "Episode: 259 | Moving Average Reward: 97 | Episode Reward: 196 | Loss: 4.484 | Steps: 197 | Worker: 1\n",
            "Episode: 260 | Moving Average Reward: 99 | Episode Reward: 310 | Loss: 3.791 | Steps: 311 | Worker: 0\n",
            "Episode: 261 | Moving Average Reward: 100 | Episode Reward: 159 | Loss: 4.891 | Steps: 160 | Worker: 1\n",
            "Episode: 262 | Moving Average Reward: 101 | Episode Reward: 235 | Loss: 4.04 | Steps: 236 | Worker: 0\n",
            "Episode: 263 | Moving Average Reward: 102 | Episode Reward: 224 | Loss: 5.454 | Steps: 225 | Worker: 1\n",
            "Episode: 264 | Moving Average Reward: 103 | Episode Reward: 171 | Loss: 5.795 | Steps: 172 | Worker: 0\n",
            "Episode: 265 | Moving Average Reward: 102 | Episode Reward: 49 | Loss: 13.532 | Steps: 50 | Worker: 1\n",
            "Episode: 266 | Moving Average Reward: 102 | Episode Reward: 85 | Loss: 10.386 | Steps: 86 | Worker: 0\n",
            "Episode: 267 | Moving Average Reward: 102 | Episode Reward: 83 | Loss: 10.132 | Steps: 84 | Worker: 1\n",
            "Episode: 268 | Moving Average Reward: 101 | Episode Reward: 40 | Loss: 21.69 | Steps: 41 | Worker: 0\n",
            "Episode: 269 | Moving Average Reward: 101 | Episode Reward: 60 | Loss: 12.783 | Steps: 61 | Worker: 0\n",
            "Episode: 270 | Moving Average Reward: 101 | Episode Reward: 125 | Loss: 6.833 | Steps: 126 | Worker: 1\n",
            "Episode: 271 | Moving Average Reward: 101 | Episode Reward: 127 | Loss: 5.787 | Steps: 128 | Worker: 0\n",
            "Episode: 272 | Moving Average Reward: 102 | Episode Reward: 143 | Loss: 5.974 | Steps: 144 | Worker: 1\n",
            "Episode: 273 | Moving Average Reward: 103 | Episode Reward: 177 | Loss: 3.717 | Steps: 178 | Worker: 1\n",
            "Episode: 274 | Moving Average Reward: 104 | Episode Reward: 246 | Loss: 3.547 | Steps: 247 | Worker: 0\n",
            "Episode: 275 | Moving Average Reward: 105 | Episode Reward: 160 | Loss: 5.218 | Steps: 161 | Worker: 1\n",
            "Episode: 276 | Moving Average Reward: 106 | Episode Reward: 287 | Loss: 3.205 | Steps: 288 | Worker: 0\n",
            "Episode: 277 | Moving Average Reward: 109 | Episode Reward: 350 | Loss: 2.716 | Steps: 351 | Worker: 1\n",
            "Episode: 278 | Moving Average Reward: 109 | Episode Reward: 105 | Loss: 8.992 | Steps: 106 | Worker: 1\n",
            "Episode: 279 | Moving Average Reward: 109 | Episode Reward: 131 | Loss: 5.718 | Steps: 132 | Worker: 1\n",
            "Episode: 280 | Moving Average Reward: 113 | Episode Reward: 491 | Loss: 2.652 | Steps: 492 | Worker: 0\n",
            "Episode: 281 | Moving Average Reward: 113 | Episode Reward: 162 | Loss: 5.914 | Steps: 163 | Worker: 0\n",
            "Episode: 282 | Moving Average Reward: 114 | Episode Reward: 215 | Loss: 3.68 | Steps: 216 | Worker: 0\n",
            "Episode: 283 | Moving Average Reward: 118 | Episode Reward: 502 | Loss: 2.792 | Steps: 503 | Worker: 1\n",
            "Saving best model to /tmp/, episode score: 502.0\n",
            "Episode: 284 | Moving Average Reward: 119 | Episode Reward: 191 | Loss: 4.368 | Steps: 192 | Worker: 1\n",
            "Episode: 285 | Moving Average Reward: 121 | Episode Reward: 336 | Loss: 2.92 | Steps: 337 | Worker: 0\n",
            "Episode: 286 | Moving Average Reward: 121 | Episode Reward: 125 | Loss: 7.237 | Steps: 126 | Worker: 0\n",
            "Episode: 287 | Moving Average Reward: 121 | Episode Reward: 146 | Loss: 5.671 | Steps: 147 | Worker: 1\n",
            "Episode: 288 | Moving Average Reward: 122 | Episode Reward: 166 | Loss: 5.403 | Steps: 167 | Worker: 0\n",
            "Episode: 289 | Moving Average Reward: 122 | Episode Reward: 175 | Loss: 4.415 | Steps: 176 | Worker: 1\n",
            "Episode: 290 | Moving Average Reward: 123 | Episode Reward: 169 | Loss: 4.431 | Steps: 170 | Worker: 0\n",
            "Episode: 291 | Moving Average Reward: 124 | Episode Reward: 221 | Loss: 4.819 | Steps: 222 | Worker: 1\n",
            "Episode: 292 | Moving Average Reward: 125 | Episode Reward: 288 | Loss: 3.275 | Steps: 289 | Worker: 0\n",
            "Episode: 293 | Moving Average Reward: 127 | Episode Reward: 291 | Loss: 3.493 | Steps: 292 | Worker: 1\n",
            "Episode: 294 | Moving Average Reward: 131 | Episode Reward: 473 | Loss: 2.429 | Steps: 474 | Worker: 1\n",
            "Episode: 295 | Moving Average Reward: 136 | Episode Reward: 682 | Loss: 2.31 | Steps: 683 | Worker: 0\n",
            "Saving best model to /tmp/, episode score: 682.0\n",
            "Episode: 296 | Moving Average Reward: 138 | Episode Reward: 282 | Loss: 4.221 | Steps: 283 | Worker: 0\n",
            "Episode: 297 | Moving Average Reward: 141 | Episode Reward: 486 | Loss: 2.673 | Steps: 487 | Worker: 1\n",
            "Episode: 298 | Moving Average Reward: 142 | Episode Reward: 254 | Loss: 3.552 | Steps: 255 | Worker: 0\n",
            "Episode: 299 | Moving Average Reward: 145 | Episode Reward: 408 | Loss: 3.933 | Steps: 409 | Worker: 1\n",
            "Episode: 300 | Moving Average Reward: 144 | Episode Reward: 109 | Loss: 8.025 | Steps: 110 | Worker: 1\n",
            "Episode: 301 | Moving Average Reward: 147 | Episode Reward: 386 | Loss: 2.772 | Steps: 387 | Worker: 0\n",
            "Episode: 302 | Moving Average Reward: 147 | Episode Reward: 170 | Loss: 4.529 | Steps: 171 | Worker: 1\n",
            "Episode: 303 | Moving Average Reward: 147 | Episode Reward: 159 | Loss: 4.003 | Steps: 160 | Worker: 0\n",
            "Episode: 304 | Moving Average Reward: 147 | Episode Reward: 158 | Loss: 3.888 | Steps: 159 | Worker: 1\n",
            "Episode: 305 | Moving Average Reward: 148 | Episode Reward: 188 | Loss: 4.289 | Steps: 189 | Worker: 0\n",
            "Episode: 306 | Moving Average Reward: 148 | Episode Reward: 143 | Loss: 5.891 | Steps: 144 | Worker: 1\n",
            "Episode: 307 | Moving Average Reward: 148 | Episode Reward: 233 | Loss: 3.315 | Steps: 234 | Worker: 0\n",
            "Episode: 308 | Moving Average Reward: 149 | Episode Reward: 226 | Loss: 6.123 | Steps: 227 | Worker: 1\n",
            "Episode: 309 | Moving Average Reward: 150 | Episode Reward: 261 | Loss: 5.434 | Steps: 262 | Worker: 1\n",
            "Episode: 310 | Moving Average Reward: 152 | Episode Reward: 353 | Loss: 2.617 | Steps: 354 | Worker: 0\n",
            "Episode: 311 | Moving Average Reward: 153 | Episode Reward: 223 | Loss: 8.06 | Steps: 224 | Worker: 0\n",
            "Episode: 312 | Moving Average Reward: 154 | Episode Reward: 289 | Loss: 5.845 | Steps: 290 | Worker: 1\n",
            "Episode: 313 | Moving Average Reward: 155 | Episode Reward: 178 | Loss: 7.335 | Steps: 179 | Worker: 1\n",
            "Episode: 314 | Moving Average Reward: 155 | Episode Reward: 220 | Loss: 8.2 | Steps: 221 | Worker: 0\n",
            "Episode: 315 | Moving Average Reward: 155 | Episode Reward: 174 | Loss: 7.171 | Steps: 175 | Worker: 0\n",
            "Episode: 316 | Moving Average Reward: 156 | Episode Reward: 228 | Loss: 6.172 | Steps: 229 | Worker: 1\n",
            "Episode: 317 | Moving Average Reward: 157 | Episode Reward: 241 | Loss: 6.426 | Steps: 242 | Worker: 0\n",
            "Episode: 318 | Moving Average Reward: 158 | Episode Reward: 258 | Loss: 4.489 | Steps: 259 | Worker: 1\n",
            "Episode: 319 | Moving Average Reward: 160 | Episode Reward: 312 | Loss: 4.128 | Steps: 313 | Worker: 0\n",
            "Episode: 320 | Moving Average Reward: 161 | Episode Reward: 310 | Loss: 4.327 | Steps: 311 | Worker: 1\n",
            "Episode: 321 | Moving Average Reward: 162 | Episode Reward: 297 | Loss: 3.856 | Steps: 298 | Worker: 0\n",
            "Episode: 322 | Moving Average Reward: 166 | Episode Reward: 501 | Loss: 3.432 | Steps: 502 | Worker: 1\n",
            "Episode: 323 | Moving Average Reward: 168 | Episode Reward: 338 | Loss: 3.529 | Steps: 339 | Worker: 0\n",
            "Episode: 324 | Moving Average Reward: 168 | Episode Reward: 231 | Loss: 5.233 | Steps: 232 | Worker: 1\n",
            "Episode: 325 | Moving Average Reward: 170 | Episode Reward: 400 | Loss: 3.985 | Steps: 401 | Worker: 0\n",
            "Episode: 326 | Moving Average Reward: 172 | Episode Reward: 278 | Loss: 3.833 | Steps: 279 | Worker: 1\n",
            "Episode: 327 | Moving Average Reward: 172 | Episode Reward: 201 | Loss: 6.252 | Steps: 202 | Worker: 0\n",
            "Episode: 328 | Moving Average Reward: 172 | Episode Reward: 211 | Loss: 5.172 | Steps: 212 | Worker: 1\n",
            "Episode: 329 | Moving Average Reward: 174 | Episode Reward: 340 | Loss: 4.16 | Steps: 341 | Worker: 0\n",
            "Episode: 330 | Moving Average Reward: 175 | Episode Reward: 332 | Loss: 3.691 | Steps: 333 | Worker: 1\n",
            "Episode: 331 | Moving Average Reward: 176 | Episode Reward: 231 | Loss: 4.711 | Steps: 232 | Worker: 0Episode: 331 | Moving Average Reward: 176 | Episode Reward: 236 | Loss: 3.913 | Steps: 237 | Worker: 1\n",
            "\n",
            "Episode: 333 | Moving Average Reward: 176 | Episode Reward: 179 | Loss: 4.74 | Steps: 180 | Worker: 1\n",
            "Episode: 334 | Moving Average Reward: 176 | Episode Reward: 173 | Loss: 5.176 | Steps: 174 | Worker: 0\n",
            "Episode: 335 | Moving Average Reward: 176 | Episode Reward: 207 | Loss: 5.004 | Steps: 208 | Worker: 1\n",
            "Episode: 336 | Moving Average Reward: 177 | Episode Reward: 260 | Loss: 4.194 | Steps: 261 | Worker: 0\n",
            "Episode: 337 | Moving Average Reward: 177 | Episode Reward: 157 | Loss: 4.309 | Steps: 158 | Worker: 1\n",
            "Episode: 338 | Moving Average Reward: 177 | Episode Reward: 140 | Loss: 6.193 | Steps: 141 | Worker: 1\n",
            "Episode: 339 | Moving Average Reward: 177 | Episode Reward: 250 | Loss: 3.195 | Steps: 251 | Worker: 0\n",
            "Episode: 340 | Moving Average Reward: 177 | Episode Reward: 152 | Loss: 4.367 | Steps: 153 | Worker: 1\n",
            "Episode: 341 | Moving Average Reward: 177 | Episode Reward: 137 | Loss: 4.554 | Steps: 138 | Worker: 0\n",
            "Episode: 342 | Moving Average Reward: 176 | Episode Reward: 139 | Loss: 4.047 | Steps: 140 | Worker: 1\n",
            "Episode: 343 | Moving Average Reward: 176 | Episode Reward: 154 | Loss: 4.039 | Steps: 155 | Worker: 0\n",
            "Episode: 344 | Moving Average Reward: 176 | Episode Reward: 136 | Loss: 3.567 | Steps: 137 | Worker: 0\n",
            "Episode: 345 | Moving Average Reward: 176 | Episode Reward: 195 | Loss: 2.922 | Steps: 196 | Worker: 1\n",
            "Episode: 346 | Moving Average Reward: 176 | Episode Reward: 159 | Loss: 3.301 | Steps: 160 | Worker: 0\n",
            "Episode: 347 | Moving Average Reward: 176 | Episode Reward: 209 | Loss: 3.138 | Steps: 210 | Worker: 1\n",
            "Episode: 348 | Moving Average Reward: 176 | Episode Reward: 194 | Loss: 2.745 | Steps: 195 | Worker: 0\n",
            "Episode: 349 | Moving Average Reward: 176 | Episode Reward: 194 | Loss: 2.726 | Steps: 195 | Worker: 1\n",
            "Episode: 350 | Moving Average Reward: 177 | Episode Reward: 213 | Loss: 2.358 | Steps: 214 | Worker: 0\n",
            "Episode: 351 | Moving Average Reward: 176 | Episode Reward: 159 | Loss: 2.509 | Steps: 160 | Worker: 1\n",
            "Episode: 352 | Moving Average Reward: 176 | Episode Reward: 173 | Loss: 2.511 | Steps: 174 | Worker: 0\n",
            "Episode: 353 | Moving Average Reward: 176 | Episode Reward: 161 | Loss: 3.346 | Steps: 162 | Worker: 1\n",
            "Episode: 354 | Moving Average Reward: 176 | Episode Reward: 152 | Loss: 2.568 | Steps: 153 | Worker: 0\n",
            "Episode: 355 | Moving Average Reward: 176 | Episode Reward: 163 | Loss: 2.859 | Steps: 164 | Worker: 1\n",
            "Episode: 356 | Moving Average Reward: 176 | Episode Reward: 160 | Loss: 2.948 | Steps: 161 | Worker: 0\n",
            "Episode: 357 | Moving Average Reward: 176 | Episode Reward: 172 | Loss: 2.102 | Steps: 173 | Worker: 1\n",
            "Episode: 358 | Moving Average Reward: 176 | Episode Reward: 164 | Loss: 2.878 | Steps: 165 | Worker: 0\n",
            "Episode: 359 | Moving Average Reward: 176 | Episode Reward: 168 | Loss: 2.153 | Steps: 169 | Worker: 1\n",
            "Episode: 360 | Moving Average Reward: 175 | Episode Reward: 144 | Loss: 3.449 | Steps: 145 | Worker: 1\n",
            "Episode: 361 | Moving Average Reward: 175 | Episode Reward: 167 | Loss: 2.102 | Steps: 168 | Worker: 0\n",
            "Episode: 362 | Moving Average Reward: 175 | Episode Reward: 139 | Loss: 1.911 | Steps: 140 | Worker: 1\n",
            "Episode: 363 | Moving Average Reward: 175 | Episode Reward: 163 | Loss: 2.643 | Steps: 164 | Worker: 0\n",
            "Episode: 364 | Moving Average Reward: 174 | Episode Reward: 125 | Loss: 2.619 | Steps: 126 | Worker: 0\n",
            "Episode: 365 | Moving Average Reward: 174 | Episode Reward: 165 | Loss: 2.048 | Steps: 166 | Worker: 1\n",
            "Episode: 366 | Moving Average Reward: 174 | Episode Reward: 134 | Loss: 2.043 | Steps: 135 | Worker: 1\n",
            "Episode: 367 | Moving Average Reward: 173 | Episode Reward: 161 | Loss: 2.449 | Steps: 162 | Worker: 0\n",
            "Episode: 368 | Moving Average Reward: 173 | Episode Reward: 159 | Loss: 1.657 | Steps: 160 | Worker: 1\n",
            "Episode: 369 | Moving Average Reward: 173 | Episode Reward: 159 | Loss: 1.547 | Steps: 160 | Worker: 0\n",
            "Episode: 370 | Moving Average Reward: 173 | Episode Reward: 149 | Loss: 3.257 | Steps: 150 | Worker: 1\n",
            "Episode: 371 | Moving Average Reward: 173 | Episode Reward: 154 | Loss: 1.924 | Steps: 155 | Worker: 0\n",
            "Episode: 372 | Moving Average Reward: 172 | Episode Reward: 128 | Loss: 2.6 | Steps: 129 | Worker: 1\n",
            "Episode: 373 | Moving Average Reward: 173 | Episode Reward: 235 | Loss: 1.685 | Steps: 236 | Worker: 0\n",
            "Episode: 374 | Moving Average Reward: 173 | Episode Reward: 223 | Loss: 2.374 | Steps: 224 | Worker: 1\n",
            "Episode: 375 | Moving Average Reward: 173 | Episode Reward: 159 | Loss: 1.892 | Steps: 160 | Worker: 1\n",
            "Episode: 376 | Moving Average Reward: 175 | Episode Reward: 348 | Loss: 1.635 | Steps: 349 | Worker: 0\n",
            "Episode: 377 | Moving Average Reward: 176 | Episode Reward: 283 | Loss: 1.733 | Steps: 284 | Worker: 1\n",
            "Episode: 378 | Moving Average Reward: 177 | Episode Reward: 248 | Loss: 2.022 | Steps: 249 | Worker: 0\n",
            "Episode: 379 | Moving Average Reward: 178 | Episode Reward: 259 | Loss: 1.609 | Steps: 260 | Worker: 1\n",
            "Episode: 380 | Moving Average Reward: 178 | Episode Reward: 200 | Loss: 2.808 | Steps: 201 | Worker: 0\n",
            "Episode: 381 | Moving Average Reward: 178 | Episode Reward: 156 | Loss: 2.12 | Steps: 157 | Worker: 1\n",
            "Episode: 382 | Moving Average Reward: 178 | Episode Reward: 241 | Loss: 2.771 | Steps: 242 | Worker: 0\n",
            "Episode: 383 | Moving Average Reward: 179 | Episode Reward: 272 | Loss: 1.629 | Steps: 273 | Worker: 1\n",
            "Episode: 384 | Moving Average Reward: 181 | Episode Reward: 392 | Loss: 1.544 | Steps: 393 | Worker: 0\n",
            "Episode: 385 | Moving Average Reward: 182 | Episode Reward: 245 | Loss: 2.337 | Steps: 246 | Worker: 1\n",
            "Episode: 386 | Moving Average Reward: 183 | Episode Reward: 312 | Loss: 1.603 | Steps: 313 | Worker: 0\n",
            "Episode: 387 | Moving Average Reward: 185 | Episode Reward: 350 | Loss: 1.548 | Steps: 351 | Worker: 1\n",
            "Episode: 388 | Moving Average Reward: 185 | Episode Reward: 224 | Loss: 2.148 | Steps: 225 | Worker: 0\n",
            "Episode: 389 | Moving Average Reward: 187 | Episode Reward: 380 | Loss: 1.634 | Steps: 381 | Worker: 1\n",
            "Episode: 390 | Moving Average Reward: 188 | Episode Reward: 317 | Loss: 1.423 | Steps: 318 | Worker: 0\n",
            "Episode: 391 | Moving Average Reward: 189 | Episode Reward: 219 | Loss: 1.52 | Steps: 220 | Worker: 1\n",
            "Episode: 392 | Moving Average Reward: 189 | Episode Reward: 187 | Loss: 1.664 | Steps: 188 | Worker: 0\n",
            "Episode: 393 | Moving Average Reward: 189 | Episode Reward: 232 | Loss: 1.469 | Steps: 233 | Worker: 1\n",
            "Episode: 394 | Moving Average Reward: 189 | Episode Reward: 219 | Loss: 1.199 | Steps: 220 | Worker: 0\n",
            "Episode: 395 | Moving Average Reward: 189 | Episode Reward: 163 | Loss: 2.045 | Steps: 164 | Worker: 1\n",
            "Episode: 396 | Moving Average Reward: 189 | Episode Reward: 170 | Loss: 1.721 | Steps: 171 | Worker: 0\n",
            "Episode: 397 | Moving Average Reward: 189 | Episode Reward: 197 | Loss: 1.412 | Steps: 198 | Worker: 1\n",
            "Episode: 398 | Moving Average Reward: 189 | Episode Reward: 173 | Loss: 1.585 | Steps: 174 | Worker: 0\n",
            "Episode: 399 | Moving Average Reward: 188 | Episode Reward: 134 | Loss: 1.605 | Steps: 135 | Worker: 1\n",
            "Episode: 400 | Moving Average Reward: 188 | Episode Reward: 155 | Loss: 1.645 | Steps: 156 | Worker: 0\n",
            "Episode: 401 | Moving Average Reward: 188 | Episode Reward: 203 | Loss: 1.835 | Steps: 204 | Worker: 1\n",
            "Episode: 402 | Moving Average Reward: 188 | Episode Reward: 183 | Loss: 1.57 | Steps: 184 | Worker: 0\n",
            "Episode: 403 | Moving Average Reward: 188 | Episode Reward: 152 | Loss: 1.568 | Steps: 153 | Worker: 1\n",
            "Episode: 404 | Moving Average Reward: 188 | Episode Reward: 173 | Loss: 1.383 | Steps: 174 | Worker: 0\n",
            "Episode: 405 | Moving Average Reward: 187 | Episode Reward: 150 | Loss: 1.298 | Steps: 151 | Worker: 1\n",
            "Episode: 406 | Moving Average Reward: 187 | Episode Reward: 164 | Loss: 1.402 | Steps: 165 | Worker: 0\n",
            "Episode: 407 | Moving Average Reward: 187 | Episode Reward: 182 | Loss: 1.54 | Steps: 183 | Worker: 1\n",
            "Episode: 408 | Moving Average Reward: 187 | Episode Reward: 222 | Loss: 1.068 | Steps: 223 | Worker: 0\n",
            "Episode: 409 | Moving Average Reward: 188 | Episode Reward: 225 | Loss: 1.243 | Steps: 226 | Worker: 1\n",
            "Episode: 410 | Moving Average Reward: 188 | Episode Reward: 207 | Loss: 1.14 | Steps: 208 | Worker: 0\n",
            "Episode: 411 | Moving Average Reward: 188 | Episode Reward: 192 | Loss: 1.049 | Steps: 193 | Worker: 1\n",
            "Episode: 412 | Moving Average Reward: 188 | Episode Reward: 154 | Loss: 1.085 | Steps: 155 | Worker: 0\n",
            "Episode: 413 | Moving Average Reward: 188 | Episode Reward: 186 | Loss: 1.086 | Steps: 187 | Worker: 1\n",
            "Episode: 414 | Moving Average Reward: 188 | Episode Reward: 201 | Loss: 1.137 | Steps: 202 | Worker: 0\n",
            "Episode: 415 | Moving Average Reward: 187 | Episode Reward: 173 | Loss: 0.993 | Steps: 174 | Worker: 1\n",
            "Episode: 416 | Moving Average Reward: 187 | Episode Reward: 166 | Loss: 1.465 | Steps: 167 | Worker: 1\n",
            "Episode: 417 | Moving Average Reward: 187 | Episode Reward: 207 | Loss: 0.958 | Steps: 208 | Worker: 0\n",
            "Episode: 418 | Moving Average Reward: 187 | Episode Reward: 180 | Loss: 1.121 | Steps: 181 | Worker: 1\n",
            "Episode: 419 | Moving Average Reward: 188 | Episode Reward: 229 | Loss: 0.858 | Steps: 230 | Worker: 0\n",
            "Episode: 420 | Moving Average Reward: 187 | Episode Reward: 138 | Loss: 1.059 | Steps: 139 | Worker: 0\n",
            "Episode: 421 | Moving Average Reward: 188 | Episode Reward: 218 | Loss: 0.807 | Steps: 219 | Worker: 1\n",
            "Episode: 422 | Moving Average Reward: 188 | Episode Reward: 249 | Loss: 0.891 | Steps: 250 | Worker: 0\n",
            "Episode: 423 | Moving Average Reward: 189 | Episode Reward: 260 | Loss: 1.029 | Steps: 261 | Worker: 1\n",
            "Episode: 424 | Moving Average Reward: 189 | Episode Reward: 156 | Loss: 0.961 | Steps: 157 | Worker: 0\n",
            "Episode: 425 | Moving Average Reward: 189 | Episode Reward: 214 | Loss: 0.742 | Steps: 215 | Worker: 1\n",
            "Episode: 426 | Moving Average Reward: 189 | Episode Reward: 182 | Loss: 1.122 | Steps: 183 | Worker: 0\n",
            "Episode: 427 | Moving Average Reward: 188 | Episode Reward: 141 | Loss: 1.284 | Steps: 142 | Worker: 1\n",
            "Episode: 428 | Moving Average Reward: 188 | Episode Reward: 180 | Loss: 0.951 | Steps: 181 | Worker: 0\n",
            "Episode: 429 | Moving Average Reward: 188 | Episode Reward: 210 | Loss: 0.618 | Steps: 211 | Worker: 1\n",
            "Episode: 430 | Moving Average Reward: 188 | Episode Reward: 154 | Loss: 0.679 | Steps: 155 | Worker: 0\n",
            "Episode: 431 | Moving Average Reward: 188 | Episode Reward: 189 | Loss: 0.745 | Steps: 190 | Worker: 1\n",
            "Episode: 432 | Moving Average Reward: 188 | Episode Reward: 157 | Loss: 0.548 | Steps: 158 | Worker: 0\n",
            "Episode: 433 | Moving Average Reward: 187 | Episode Reward: 122 | Loss: 1.052 | Steps: 123 | Worker: 0Episode: 433 | Moving Average Reward: 187 | Episode Reward: 145 | Loss: 0.843 | Steps: 146 | Worker: 1\n",
            "\n",
            "Episode: 435 | Moving Average Reward: 187 | Episode Reward: 143 | Loss: 4.3 | Steps: 144 | Worker: 1\n",
            "Episode: 436 | Moving Average Reward: 187 | Episode Reward: 188 | Loss: 0.594 | Steps: 189 | Worker: 0\n",
            "Episode: 437 | Moving Average Reward: 187 | Episode Reward: 204 | Loss: 0.701 | Steps: 205 | Worker: 1\n",
            "Episode: 438 | Moving Average Reward: 187 | Episode Reward: 178 | Loss: 0.504 | Steps: 179 | Worker: 0\n",
            "Episode: 439 | Moving Average Reward: 186 | Episode Reward: 128 | Loss: 0.729 | Steps: 129 | Worker: 0\n",
            "Episode: 440 | Moving Average Reward: 186 | Episode Reward: 213 | Loss: 0.546 | Steps: 214 | Worker: 1\n",
            "Episode: 441 | Moving Average Reward: 186 | Episode Reward: 144 | Loss: 0.649 | Steps: 145 | Worker: 0\n",
            "Episode: 442 | Moving Average Reward: 186 | Episode Reward: 152 | Loss: 0.667 | Steps: 153 | Worker: 0\n",
            "Episode: 443 | Moving Average Reward: 186 | Episode Reward: 243 | Loss: 0.624 | Steps: 244 | Worker: 1\n",
            "Episode: 444 | Moving Average Reward: 186 | Episode Reward: 188 | Loss: 0.581 | Steps: 189 | Worker: 1\n",
            "Episode: 445 | Moving Average Reward: 187 | Episode Reward: 257 | Loss: 0.517 | Steps: 258 | Worker: 0\n",
            "Episode: 446 | Moving Average Reward: 186 | Episode Reward: 134 | Loss: 0.595 | Steps: 135 | Worker: 1\n",
            "Episode: 447 | Moving Average Reward: 186 | Episode Reward: 190 | Loss: 0.611 | Steps: 191 | Worker: 0\n",
            "Episode: 448 | Moving Average Reward: 186 | Episode Reward: 173 | Loss: 0.608 | Steps: 174 | Worker: 1\n",
            "Episode: 449 | Moving Average Reward: 186 | Episode Reward: 160 | Loss: 0.98 | Steps: 161 | Worker: 0\n",
            "Episode: 450 | Moving Average Reward: 186 | Episode Reward: 149 | Loss: 0.707 | Steps: 150 | Worker: 1\n",
            "Episode: 451 | Moving Average Reward: 186 | Episode Reward: 187 | Loss: 0.489 | Steps: 188 | Worker: 0\n",
            "Episode: 452 | Moving Average Reward: 186 | Episode Reward: 181 | Loss: 0.566 | Steps: 182 | Worker: 1\n",
            "Episode: 453 | Moving Average Reward: 186 | Episode Reward: 216 | Loss: 0.408 | Steps: 217 | Worker: 0\n",
            "Episode: 454 | Moving Average Reward: 185 | Episode Reward: 147 | Loss: 0.573 | Steps: 148 | Worker: 1\n",
            "Episode: 455 | Moving Average Reward: 185 | Episode Reward: 166 | Loss: 0.512 | Steps: 167 | Worker: 0\n",
            "Episode: 456 | Moving Average Reward: 186 | Episode Reward: 221 | Loss: 0.493 | Steps: 222 | Worker: 1\n",
            "Episode: 457 | Moving Average Reward: 185 | Episode Reward: 141 | Loss: 0.626 | Steps: 142 | Worker: 0\n",
            "Episode: 458 | Moving Average Reward: 185 | Episode Reward: 135 | Loss: 2.984 | Steps: 136 | Worker: 1\n",
            "Episode: 459 | Moving Average Reward: 184 | Episode Reward: 162 | Loss: 0.714 | Steps: 163 | Worker: 1\n",
            "Episode: 460 | Moving Average Reward: 185 | Episode Reward: 215 | Loss: 0.418 | Steps: 216 | Worker: 0\n",
            "Episode: 461 | Moving Average Reward: 184 | Episode Reward: 157 | Loss: 0.448 | Steps: 158 | Worker: 1\n",
            "Episode: 462 | Moving Average Reward: 186 | Episode Reward: 298 | Loss: 0.413 | Steps: 299 | Worker: 0\n",
            "Episode: 463 | Moving Average Reward: 186 | Episode Reward: 208 | Loss: 0.49 | Steps: 209 | Worker: 1\n",
            "Episode: 464 | Moving Average Reward: 186 | Episode Reward: 243 | Loss: 0.537 | Steps: 244 | Worker: 1\n",
            "Episode: 465 | Moving Average Reward: 187 | Episode Reward: 284 | Loss: 0.456 | Steps: 285 | Worker: 0\n",
            "Episode: 466 | Moving Average Reward: 187 | Episode Reward: 161 | Loss: 0.632 | Steps: 162 | Worker: 0\n",
            "Episode: 467 | Moving Average Reward: 188 | Episode Reward: 301 | Loss: 0.497 | Steps: 302 | Worker: 1\n",
            "Episode: 468 | Moving Average Reward: 189 | Episode Reward: 224 | Loss: 0.452 | Steps: 225 | Worker: 0\n",
            "Episode: 469 | Moving Average Reward: 189 | Episode Reward: 232 | Loss: 0.432 | Steps: 233 | Worker: 1\n",
            "Episode: 470 | Moving Average Reward: 189 | Episode Reward: 151 | Loss: 0.606 | Steps: 152 | Worker: 1\n",
            "Episode: 471 | Moving Average Reward: 190 | Episode Reward: 318 | Loss: 0.385 | Steps: 319 | Worker: 0\n",
            "Episode: 472 | Moving Average Reward: 190 | Episode Reward: 201 | Loss: 0.629 | Steps: 202 | Worker: 0\n",
            "Episode: 473 | Moving Average Reward: 192 | Episode Reward: 353 | Loss: 0.387 | Steps: 354 | Worker: 1\n",
            "Episode: 474 | Moving Average Reward: 191 | Episode Reward: 173 | Loss: 0.395 | Steps: 174 | Worker: 1\n",
            "Episode: 475 | Moving Average Reward: 191 | Episode Reward: 185 | Loss: 0.467 | Steps: 186 | Worker: 0\n",
            "Episode: 476 | Moving Average Reward: 192 | Episode Reward: 226 | Loss: 0.532 | Steps: 227 | Worker: 1\n",
            "Episode: 477 | Moving Average Reward: 192 | Episode Reward: 224 | Loss: 0.506 | Steps: 225 | Worker: 0\n",
            "Episode: 478 | Moving Average Reward: 192 | Episode Reward: 223 | Loss: 0.47 | Steps: 224 | Worker: 1\n",
            "Episode: 479 | Moving Average Reward: 192 | Episode Reward: 191 | Loss: 0.48 | Steps: 192 | Worker: 0\n",
            "Episode: 480 | Moving Average Reward: 192 | Episode Reward: 142 | Loss: 0.599 | Steps: 143 | Worker: 1\n",
            "Episode: 481 | Moving Average Reward: 193 | Episode Reward: 299 | Loss: 0.332 | Steps: 300 | Worker: 0\n",
            "Episode: 482 | Moving Average Reward: 193 | Episode Reward: 225 | Loss: 0.543 | Steps: 226 | Worker: 1\n",
            "Episode: 483 | Moving Average Reward: 193 | Episode Reward: 153 | Loss: 0.842 | Steps: 154 | Worker: 0\n",
            "Episode: 484 | Moving Average Reward: 192 | Episode Reward: 152 | Loss: 0.501 | Steps: 153 | Worker: 1\n",
            "Episode: 485 | Moving Average Reward: 192 | Episode Reward: 152 | Loss: 0.443 | Steps: 153 | Worker: 1Episode: 485 | Moving Average Reward: 192 | Episode Reward: 205 | Loss: 0.421 | Steps: 206 | Worker: 0\n",
            "\n",
            "Episode: 487 | Moving Average Reward: 193 | Episode Reward: 259 | Loss: 0.347 | Steps: 260 | Worker: 1\n",
            "Episode: 488 | Moving Average Reward: 194 | Episode Reward: 337 | Loss: 0.567 | Steps: 338 | Worker: 0\n",
            "Episode: 489 | Moving Average Reward: 194 | Episode Reward: 187 | Loss: 0.493 | Steps: 188 | Worker: 1\n",
            "Episode: 490 | Moving Average Reward: 195 | Episode Reward: 249 | Loss: 0.412 | Steps: 250 | Worker: 0\n",
            "Episode: 491 | Moving Average Reward: 196 | Episode Reward: 351 | Loss: 0.347 | Steps: 352 | Worker: 1\n",
            "Episode: 492 | Moving Average Reward: 197 | Episode Reward: 288 | Loss: 21.464 | Steps: 289 | Worker: 0\n",
            "Episode: 493 | Moving Average Reward: 198 | Episode Reward: 266 | Loss: 26.048 | Steps: 267 | Worker: 1\n",
            "Episode: 494 | Moving Average Reward: 199 | Episode Reward: 287 | Loss: 16.205 | Steps: 288 | Worker: 0\n",
            "Episode: 495 | Moving Average Reward: 199 | Episode Reward: 281 | Loss: 0.661 | Steps: 282 | Worker: 1\n",
            "Episode: 496 | Moving Average Reward: 198 | Episode Reward: 100 | Loss: 56.925 | Steps: 101 | Worker: 1\n",
            "Episode: 497 | Moving Average Reward: 201 | Episode Reward: 506 | Loss: 0.657 | Steps: 507 | Worker: 0\n",
            "Episode: 498 | Moving Average Reward: 201 | Episode Reward: 172 | Loss: 25.986 | Steps: 173 | Worker: 1\n",
            "Episode: 499 | Moving Average Reward: 202 | Episode Reward: 254 | Loss: 15.909 | Steps: 255 | Worker: 0\n",
            "Episode: 500 | Moving Average Reward: 202 | Episode Reward: 241 | Loss: 1.02 | Steps: 242 | Worker: 1\n",
            "Episode: 501 | Moving Average Reward: 203 | Episode Reward: 295 | Loss: 12.496 | Steps: 296 | Worker: 1\n",
            "Episode: 502 | Moving Average Reward: 205 | Episode Reward: 419 | Loss: 8.066 | Steps: 420 | Worker: 0\n",
            "Episode: 503 | Moving Average Reward: 206 | Episode Reward: 256 | Loss: 12.869 | Steps: 257 | Worker: 1\n",
            "Episode: 504 | Moving Average Reward: 207 | Episode Reward: 293 | Loss: 10.662 | Steps: 294 | Worker: 1\n",
            "Episode: 505 | Moving Average Reward: 210 | Episode Reward: 517 | Loss: 5.779 | Steps: 518 | Worker: 0\n",
            "Episode: 506 | Moving Average Reward: 210 | Episode Reward: 241 | Loss: 14.296 | Steps: 242 | Worker: 0\n",
            "Episode: 507 | Moving Average Reward: 213 | Episode Reward: 517 | Loss: 1.041 | Steps: 518 | Worker: 1\n",
            "Episode: 508 | Moving Average Reward: 214 | Episode Reward: 275 | Loss: 9.363 | Steps: 276 | Worker: 0\n",
            "Episode: 509 | Moving Average Reward: 214 | Episode Reward: 264 | Loss: 2.104 | Steps: 265 | Worker: 0\n",
            "Episode: 510 | Moving Average Reward: 216 | Episode Reward: 374 | Loss: 1.307 | Steps: 375 | Worker: 1\n",
            "Episode: 511 | Moving Average Reward: 217 | Episode Reward: 297 | Loss: 8.393 | Steps: 298 | Worker: 0\n",
            "Episode: 512 | Moving Average Reward: 218 | Episode Reward: 410 | Loss: 1.204 | Steps: 411 | Worker: 0\n",
            "Episode: 513 | Moving Average Reward: 223 | Episode Reward: 645 | Loss: 4.664 | Steps: 646 | Worker: 1\n",
            "Episode: 514 | Moving Average Reward: 223 | Episode Reward: 231 | Loss: 2.247 | Steps: 232 | Worker: 0\n",
            "Episode: 515 | Moving Average Reward: 224 | Episode Reward: 389 | Loss: 1.838 | Steps: 390 | Worker: 1\n",
            "Episode: 516 | Moving Average Reward: 225 | Episode Reward: 284 | Loss: 1.975 | Steps: 285 | Worker: 0\n",
            "Episode: 517 | Moving Average Reward: 225 | Episode Reward: 267 | Loss: 1.571 | Steps: 268 | Worker: 1\n",
            "Episode: 518 | Moving Average Reward: 225 | Episode Reward: 206 | Loss: 2.104 | Steps: 207 | Worker: 0\n",
            "Episode: 519 | Moving Average Reward: 225 | Episode Reward: 206 | Loss: 1.847 | Steps: 207 | Worker: 0\n",
            "Episode: 520 | Moving Average Reward: 226 | Episode Reward: 307 | Loss: 1.519 | Steps: 308 | Worker: 1\n",
            "Episode: 521 | Moving Average Reward: 227 | Episode Reward: 305 | Loss: 1.409 | Steps: 306 | Worker: 0\n",
            "Episode: 522 | Moving Average Reward: 228 | Episode Reward: 353 | Loss: 1.016 | Steps: 354 | Worker: 1\n",
            "Episode: 523 | Moving Average Reward: 227 | Episode Reward: 186 | Loss: 1.7 | Steps: 187 | Worker: 0\n",
            "Episode: 524 | Moving Average Reward: 228 | Episode Reward: 234 | Loss: 1.296 | Steps: 235 | Worker: 1\n",
            "Episode: 525 | Moving Average Reward: 228 | Episode Reward: 255 | Loss: 1.008 | Steps: 256 | Worker: 0\n",
            "Episode: 526 | Moving Average Reward: 227 | Episode Reward: 183 | Loss: 1.356 | Steps: 184 | Worker: 1\n",
            "Episode: 527 | Moving Average Reward: 227 | Episode Reward: 204 | Loss: 1.089 | Steps: 205 | Worker: 0\n",
            "Episode: 528 | Moving Average Reward: 227 | Episode Reward: 215 | Loss: 0.867 | Steps: 216 | Worker: 1\n",
            "Episode: 529 | Moving Average Reward: 227 | Episode Reward: 216 | Loss: 0.967 | Steps: 217 | Worker: 0\n",
            "Episode: 530 | Moving Average Reward: 227 | Episode Reward: 234 | Loss: 1.088 | Steps: 235 | Worker: 1\n",
            "Episode: 531 | Moving Average Reward: 227 | Episode Reward: 223 | Loss: 1.039 | Steps: 224 | Worker: 0\n",
            "Episode: 532 | Moving Average Reward: 226 | Episode Reward: 179 | Loss: 0.997 | Steps: 180 | Worker: 1\n",
            "Episode: 533 | Moving Average Reward: 226 | Episode Reward: 179 | Loss: 0.8 | Steps: 180 | Worker: 0\n",
            "Episode: 534 | Moving Average Reward: 226 | Episode Reward: 190 | Loss: 1.135 | Steps: 191 | Worker: 1\n",
            "Episode: 535 | Moving Average Reward: 226 | Episode Reward: 235 | Loss: 0.695 | Steps: 236 | Worker: 0\n",
            "Episode: 536 | Moving Average Reward: 226 | Episode Reward: 223 | Loss: 1.103 | Steps: 224 | Worker: 1\n",
            "Episode: 537 | Moving Average Reward: 226 | Episode Reward: 221 | Loss: 1.072 | Steps: 222 | Worker: 0\n",
            "Episode: 538 | Moving Average Reward: 225 | Episode Reward: 214 | Loss: 0.969 | Steps: 215 | Worker: 1\n",
            "Episode: 539 | Moving Average Reward: 225 | Episode Reward: 175 | Loss: 0.852 | Steps: 176 | Worker: 1\n",
            "Episode: 540 | Moving Average Reward: 225 | Episode Reward: 270 | Loss: 0.604 | Steps: 271 | Worker: 0\n",
            "Episode: 541 | Moving Average Reward: 226 | Episode Reward: 236 | Loss: 0.537 | Steps: 237 | Worker: 1\n",
            "Episode: 542 | Moving Average Reward: 226 | Episode Reward: 234 | Loss: 0.465 | Steps: 235 | Worker: 0\n",
            "Episode: 543 | Moving Average Reward: 225 | Episode Reward: 172 | Loss: 0.739 | Steps: 173 | Worker: 1\n",
            "Episode: 544 | Moving Average Reward: 224 | Episode Reward: 136 | Loss: 0.666 | Steps: 137 | Worker: 0\n",
            "Episode: 545 | Moving Average Reward: 224 | Episode Reward: 162 | Loss: 0.859 | Steps: 163 | Worker: 1\n",
            "Episode: 546 | Moving Average Reward: 223 | Episode Reward: 204 | Loss: 0.658 | Steps: 205 | Worker: 0\n",
            "Episode: 547 | Moving Average Reward: 223 | Episode Reward: 156 | Loss: 0.792 | Steps: 157 | Worker: 1\n",
            "Episode: 548 | Moving Average Reward: 222 | Episode Reward: 188 | Loss: 0.501 | Steps: 189 | Worker: 0\n",
            "Episode: 549 | Moving Average Reward: 222 | Episode Reward: 168 | Loss: 0.976 | Steps: 169 | Worker: 1\n",
            "Episode: 550 | Moving Average Reward: 222 | Episode Reward: 213 | Loss: 0.846 | Steps: 214 | Worker: 0\n",
            "Episode: 551 | Moving Average Reward: 223 | Episode Reward: 375 | Loss: 0.517 | Steps: 376 | Worker: 1\n",
            "Episode: 552 | Moving Average Reward: 223 | Episode Reward: 244 | Loss: 0.792 | Steps: 245 | Worker: 0\n",
            "Episode: 553 | Moving Average Reward: 223 | Episode Reward: 201 | Loss: 0.702 | Steps: 202 | Worker: 1\n",
            "Episode: 554 | Moving Average Reward: 224 | Episode Reward: 280 | Loss: 0.728 | Steps: 281 | Worker: 0\n",
            "Episode: 555 | Moving Average Reward: 225 | Episode Reward: 332 | Loss: 0.474 | Steps: 333 | Worker: 1\n",
            "Episode: 556 | Moving Average Reward: 225 | Episode Reward: 237 | Loss: 0.61 | Steps: 238 | Worker: 0\n",
            "Episode: 557 | Moving Average Reward: 225 | Episode Reward: 221 | Loss: 0.517 | Steps: 222 | Worker: 1\n",
            "Episode: 558 | Moving Average Reward: 225 | Episode Reward: 204 | Loss: 0.639 | Steps: 205 | Worker: 0\n",
            "Episode: 559 | Moving Average Reward: 224 | Episode Reward: 167 | Loss: 0.483 | Steps: 168 | Worker: 1\n",
            "Episode: 560 | Moving Average Reward: 224 | Episode Reward: 243 | Loss: 0.502 | Steps: 244 | Worker: 0\n",
            "Episode: 561 | Moving Average Reward: 224 | Episode Reward: 200 | Loss: 0.496 | Steps: 201 | Worker: 1\n",
            "Episode: 562 | Moving Average Reward: 223 | Episode Reward: 156 | Loss: 0.427 | Steps: 157 | Worker: 0\n",
            "Episode: 563 | Moving Average Reward: 223 | Episode Reward: 225 | Loss: 0.492 | Steps: 226 | Worker: 0\n",
            "Episode: 564 | Moving Average Reward: 224 | Episode Reward: 299 | Loss: 0.364 | Steps: 300 | Worker: 1\n",
            "Episode: 565 | Moving Average Reward: 224 | Episode Reward: 170 | Loss: 0.46 | Steps: 171 | Worker: 1\n",
            "Episode: 566 | Moving Average Reward: 224 | Episode Reward: 271 | Loss: 0.342 | Steps: 272 | Worker: 0\n",
            "Episode: 567 | Moving Average Reward: 224 | Episode Reward: 175 | Loss: 0.477 | Steps: 176 | Worker: 1\n",
            "Episode: 568 | Moving Average Reward: 223 | Episode Reward: 197 | Loss: 0.411 | Steps: 198 | Worker: 0\n",
            "Episode: 569 | Moving Average Reward: 221 | Episode Reward: 27 | Loss: 80.723 | Steps: 28 | Worker: 0\n",
            "Episode: 570 | Moving Average Reward: 221 | Episode Reward: 232 | Loss: 0.375 | Steps: 233 | Worker: 1\n",
            "Episode: 571 | Moving Average Reward: 222 | Episode Reward: 272 | Loss: 0.455 | Steps: 273 | Worker: 0\n",
            "Episode: 572 | Moving Average Reward: 224 | Episode Reward: 416 | Loss: 0.365 | Steps: 417 | Worker: 1\n",
            "Episode: 573 | Moving Average Reward: 224 | Episode Reward: 280 | Loss: 14.929 | Steps: 281 | Worker: 0\n",
            "Episode: 574 | Moving Average Reward: 225 | Episode Reward: 242 | Loss: 0.708 | Steps: 243 | Worker: 1\n",
            "Episode: 575 | Moving Average Reward: 226 | Episode Reward: 419 | Loss: 0.45 | Steps: 420 | Worker: 0\n",
            "Episode: 576 | Moving Average Reward: 227 | Episode Reward: 245 | Loss: 0.624 | Steps: 246 | Worker: 1\n",
            "Episode: 577 | Moving Average Reward: 227 | Episode Reward: 255 | Loss: 0.507 | Steps: 256 | Worker: 0\n",
            "Episode: 578 | Moving Average Reward: 227 | Episode Reward: 232 | Loss: 0.477 | Steps: 233 | Worker: 1\n",
            "Episode: 579 | Moving Average Reward: 226 | Episode Reward: 177 | Loss: 0.465 | Steps: 178 | Worker: 0\n",
            "Episode: 580 | Moving Average Reward: 227 | Episode Reward: 241 | Loss: 0.565 | Steps: 242 | Worker: 1\n",
            "Episode: 581 | Moving Average Reward: 226 | Episode Reward: 177 | Loss: 0.603 | Steps: 178 | Worker: 0\n",
            "Episode: 582 | Moving Average Reward: 226 | Episode Reward: 172 | Loss: 0.61 | Steps: 173 | Worker: 1\n",
            "Episode: 583 | Moving Average Reward: 225 | Episode Reward: 156 | Loss: 0.487 | Steps: 157 | Worker: 0\n",
            "Episode: 584 | Moving Average Reward: 224 | Episode Reward: 151 | Loss: 0.618 | Steps: 152 | Worker: 1\n",
            "Episode: 585 | Moving Average Reward: 224 | Episode Reward: 224 | Loss: 0.541 | Steps: 225 | Worker: 0\n",
            "Episode: 586 | Moving Average Reward: 224 | Episode Reward: 203 | Loss: 0.522 | Steps: 204 | Worker: 1\n",
            "Episode: 587 | Moving Average Reward: 223 | Episode Reward: 181 | Loss: 1.148 | Steps: 182 | Worker: 0\n",
            "Episode: 588 | Moving Average Reward: 223 | Episode Reward: 189 | Loss: 0.442 | Steps: 190 | Worker: 1\n",
            "Episode: 589 | Moving Average Reward: 223 | Episode Reward: 237 | Loss: 0.537 | Steps: 238 | Worker: 0\n",
            "Episode: 590 | Moving Average Reward: 223 | Episode Reward: 223 | Loss: 0.414 | Steps: 224 | Worker: 1\n",
            "Episode: 591 | Moving Average Reward: 223 | Episode Reward: 193 | Loss: 0.482 | Steps: 194 | Worker: 0\n",
            "Episode: 592 | Moving Average Reward: 223 | Episode Reward: 277 | Loss: 0.384 | Steps: 278 | Worker: 1\n",
            "Episode: 593 | Moving Average Reward: 223 | Episode Reward: 202 | Loss: 0.526 | Steps: 203 | Worker: 0\n",
            "Episode: 594 | Moving Average Reward: 223 | Episode Reward: 195 | Loss: 0.396 | Steps: 196 | Worker: 1\n",
            "Episode: 595 | Moving Average Reward: 223 | Episode Reward: 249 | Loss: 0.513 | Steps: 250 | Worker: 0\n",
            "Episode: 596 | Moving Average Reward: 223 | Episode Reward: 202 | Loss: 0.924 | Steps: 203 | Worker: 1\n",
            "Episode: 597 | Moving Average Reward: 223 | Episode Reward: 236 | Loss: 0.471 | Steps: 237 | Worker: 0\n",
            "Episode: 598 | Moving Average Reward: 223 | Episode Reward: 177 | Loss: 0.571 | Steps: 178 | Worker: 0\n",
            "Episode: 599 | Moving Average Reward: 223 | Episode Reward: 294 | Loss: 0.504 | Steps: 295 | Worker: 1\n",
            "Episode: 600 | Moving Average Reward: 224 | Episode Reward: 290 | Loss: 0.433 | Steps: 291 | Worker: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNmFhBGZT8ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## testing the algorithm\n",
        "\n",
        "# master = MasterAgent()\n",
        "# master.play()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNckp9AXP3Pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c72e65ba-a729-4a15-eb82-4b871ded2815"
      },
      "source": [
        "## evaluation video\n",
        "\n",
        "MODEL_DIR = '/tmp/model_CartPole-v0.h5'\n",
        "fps = 30\n",
        "\n",
        "# initialize\n",
        "# print(gym.make(ENV))\n",
        "env = gym.make(ENV).unwrapped\n",
        "# print(env)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "model = ActorCriticModel(state_size, action_size)\n",
        "# why do we need below\n",
        "model(tf.convert_to_tensor(np.random.random((1, state_size)),\n",
        "                           dtype = tf.float32))\n",
        "model_path = MODEL_DIR\n",
        "print('Loading model from: {}'.format(model_path))\n",
        "model.load_weights(model_path)\n",
        "\n",
        "with imageio.get_writer(VIDEO_DIR_02, fps = fps) as video:\n",
        "    state = env.reset()\n",
        "    # print(state, state.shape)\n",
        "    # print(state[None, :], state[None, :].shape)\n",
        "    done = False\n",
        "    screen = env.render(mode = 'rgb_array')\n",
        "    video.append_data(screen)\n",
        "\n",
        "    while not done:\n",
        "        # state[None, :] makes original (4,) shape to (1, 4) shape\n",
        "        policy, value = model(tf.convert_to_tensor(state[None, :], dtype = tf.float32))\n",
        "        policy = tf.nn.softmax(policy)\n",
        "        action = np.argmax(policy)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        screen = env.render(mode = 'rgb_array')\n",
        "        video.append_data(screen)\n",
        "        state = next_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model from: /tmp/model_CartPole-v0.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GleHV9fbbmlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## log\n",
        "\n",
        "# 4 2\n",
        "# Starting worker 0\n",
        "# Starting worker 1\n",
        "# Exception in thread Thread-10:\n",
        "# Traceback (most recent call last):\n",
        "#   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
        "#     self.run()\n",
        "#   File \"<ipython-input-64-de12702797f5>\", line 78, in run\n",
        "#     args.gamma)\n",
        "#   File \"<ipython-input-64-de12702797f5>\", line 150, in compute_loss\n",
        "#     dtype = tf.float32 - values\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1155, in r_binary_op_wrapper\n",
        "#     x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\n",
        "#     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 338, in _constant_tensor_conversion_function\n",
        "#     return constant(v, dtype=dtype, name=name)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n",
        "#     allow_broadcast=True)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\n",
        "#     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\n",
        "#     t = convert_to_eager_tensor(value, ctx, dtype)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n",
        "#     return ops.EagerTensor(value, ctx.device_name, dtype)\n",
        "# ValueError: Attempt to convert a value (tf.float32) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.\n",
        "\n",
        "# Exception in thread Thread-11:\n",
        "# Traceback (most recent call last):\n",
        "#   File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
        "#     self.run()\n",
        "#   File \"<ipython-input-64-de12702797f5>\", line 78, in run\n",
        "#     args.gamma)\n",
        "#   File \"<ipython-input-64-de12702797f5>\", line 150, in compute_loss\n",
        "#     dtype = tf.float32 - values\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1155, in r_binary_op_wrapper\n",
        "#     x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\n",
        "#     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 338, in _constant_tensor_conversion_function\n",
        "#     return constant(v, dtype=dtype, name=name)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n",
        "#     allow_broadcast=True)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\n",
        "#     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\n",
        "#     t = convert_to_eager_tensor(value, ctx, dtype)\n",
        "#   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n",
        "#     return ops.EagerTensor(value, ctx.device_name, dtype)\n",
        "# ValueError: Attempt to convert a value (tf.float32) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}