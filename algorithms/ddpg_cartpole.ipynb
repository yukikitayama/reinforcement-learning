{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg9tD72-9YQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## setup\n",
        "\n",
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, \\\n",
        "    Concatenate\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsT6NnA9mXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## hyperparameters\n",
        "\n",
        "# https://gym.openai.com/envs/Pendulum-v0/\n",
        "ENV = 'Pendulum-v0'\n",
        "THETA = 0.15\n",
        "DT = 1e-2\n",
        "BUFFER_CAPACITY = 100000\n",
        "BATCH_SIZE = 64\n",
        "STD = 0.2\n",
        "CRITIC_LR = 0.002\n",
        "ACTOR_LR = 0.001\n",
        "EPISODES = 100\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmabnznf9yzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7ba5b898-fcdb-4282-9456-ba60f253684e"
      },
      "source": [
        "## environment\n",
        "\n",
        "env= gym.make(ENV)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.shape[0]\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(f'state space: {num_states}')\n",
        "print(f'action space: {num_actions}')\n",
        "print(f'continuous action max: {upper_bound}')\n",
        "print(f'continuous action min: {lower_bound}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state space: 3\n",
            "action space: 1\n",
            "continuous action max: 2.0\n",
            "continuous action min: -2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suuHxDw3_b88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ornstein-uhlenbeck process\n",
        "\n",
        "class OUActionNoise:\n",
        "    \"\"\"\n",
        "    Ornstein-Uhlenbeck process models the exploration noise process\n",
        "    Use temporally correlated noise in order to explore well \n",
        "    in physical environments that have momentum.\n",
        "    In paper, theta = 0.1g, sigma = 0.2\n",
        "    \n",
        "    https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\n",
        "\n",
        "    dt = derivative of t, time\n",
        "    \"\"\"\n",
        "    # x_initial?\n",
        "    def __init__(self, mean, std, theta = THETA, dt = DT, x_initial = None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = (\n",
        "            self.x_prev\n",
        "             + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "             + self.std * np.sqrt(self.dt) * np.random.normal(size = self.mean.shape)\n",
        "        )\n",
        "        # it makes next noise dependent on current noise\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        # default x_initial is None\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFWDw0G0sMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## actor network and critic network\n",
        "\n",
        "def get_actor():\n",
        "    \"\"\"\n",
        "    Actor updates the policy distribution\n",
        "    The value is used to selection action. Here it is continuous\n",
        "\n",
        "    Makes tf.keras.Model by Functional API.\n",
        "\n",
        "    Initialize for the last layer of the Actor to be between -0.003 and 0.003\n",
        "    This prevents us from getting 1 or -1 output values in the initial stages.\n",
        "    1 or -1 would squash our gradients to zero, as using tanh activation\n",
        "    \n",
        "    kernel_initializer defines the way to set the initial random weights of Keras layers\n",
        "    tanh, hyperbolic tangent activation function producs numbers between -1 and 1.\n",
        "    Because Pendulum environment has action space -2 to 2, outputs are multipled by upper_bound (2)\n",
        "    \"\"\"\n",
        "    last_init = tf.random_uniform_initializer(minval = -0.003, maxval = 0.003)\n",
        "    inputs = Input(shape = (num_states,))\n",
        "    out = Dense(512, activation = 'relu')(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Dense(512, activation = 'relu')(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    outputs = Dense(1, activation = 'tanh', kernel_initializer = last_init)(out)\n",
        "    outputs = outputs * upper_bound\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def get_critic():\n",
        "    \"\"\"\n",
        "    Critic estimates value function (Either action-value or state-value)\n",
        "    Makes tf.keras.Model by Functional API\n",
        "    \"\"\"\n",
        "    # state as input\n",
        "    state_input = Input(shape = (num_states))\n",
        "    state_out = Dense(16, activation = 'relu')(state_input)\n",
        "    state_out = BatchNormalization()(state_out)\n",
        "    state_out = Dense(32, activation = 'relu')(state_out)\n",
        "    state_out = BatchNormalization()(state_out)\n",
        "    # action as input\n",
        "    action_input = Input(shape = (num_actions))\n",
        "    action_out = Dense(32, activation = 'relu')(action_input)\n",
        "    action_out = BatchNormalization()(action_out)\n",
        "    # both are passed through separate layers before concatenating\n",
        "    concat = Concatenate()([state_out, action_out])\n",
        "    out = Dense(512, activation = 'relu')(concat)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Dense(512, activation = 'relu')(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    outputs = Dense(1)(out)\n",
        "    model = Model([state_input, action_input], outputs)\n",
        "    return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNoTtmKKXeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## experience replay buffer\n",
        "\n",
        "class Buffer:\n",
        "    \"\"\"\n",
        "    Experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_capacity = BUFFER_CAPACITY, batch_size = BATCH_SIZE):\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.batch_size = batch_size\n",
        "        # initialize buffer_counter which is incremented by record method\n",
        "        self.buffer_counter = 0\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    def record(self, obs_tuple):\n",
        "        \"\"\"\n",
        "        When buffer_counter > buffer_capacity,\n",
        "        index has a new index starting from 0 by %\n",
        "        \"\"\"\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        This method computes the loss and update the parameters\n",
        "        \"\"\"\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # what is the shape of these?\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        # why?\n",
        "        reward_batch = tf.cast(reward_batch, dtype = tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        # update Critic\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch)\n",
        "            # critic([state_input, action_input])\n",
        "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
        "            critic_value = critic_model([state_batch, action_batch])\n",
        "            # loss function calculates mean squared loss\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "        # calculate gradient\n",
        "        critic_gradient = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        # Adam optimizer and sums gradients\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_gradient, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # update Actor\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch)\n",
        "            critic_value = critic_model([state_batch, actions])\n",
        "            # negative value because we want to maximize value given by Critic?\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "        # calculate gradient\n",
        "        actor_gradient = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        # Adam optimizer\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_gradient, actor_model.trainable_variables)\n",
        "        )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV0AVjuW_FQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## helper functions\n",
        "\n",
        "def update_target(tau):\n",
        "    \"\"\"\n",
        "    This updates target network parameters slowly.\n",
        "    \"\"\"\n",
        "    # update target Critic\n",
        "    new_weights = []\n",
        "    target_variables = target_critic.weights\n",
        "    for i, variable in enumerate(critic_model.weights):\n",
        "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
        "    target_critic.set_weights(new_weights)\n",
        "\n",
        "    # update target Actor\n",
        "    new_weights = []\n",
        "    target_variables = target_actor.weights\n",
        "    for i, variable in enumerate(actor_model.weights):\n",
        "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
        "    target_actor.set_weights(new_weights)\n",
        "\n",
        "def policy(state, noise_object):\n",
        "    \"\"\"\n",
        "    This returns an action sampled from Actor network plus noise for exploration.\n",
        "    \"\"\"\n",
        "    # tf.squeeze removes dimensions of size 1 from the shape of tensor\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # add noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "    # make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFU1WV3eOaXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## training\n",
        "\n",
        "# initialize\n",
        "ou_noise = OUActionNoise(mean = np.zeros(1), std = float(STD))\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "critic_optimizer = Adam(CRITIC_LR)\n",
        "actor_optimizer = Adam(ACTOR_LR)\n",
        "buffer = Buffer(BUFFER_CAPACITY, BATCH_SIZE)\n",
        "# store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# store average reward history of last few episodes\n",
        "avg_reward_list = []"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhWPa1UIPgry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## test get_weights()\n",
        "\n",
        "# print(type(actor_model.get_weights()))\n",
        "# print(len(actor_model.get_weights()))\n",
        "# print(actor_model.get_weights()[0].shape)"
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}