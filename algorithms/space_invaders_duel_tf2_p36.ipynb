{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWy6boLd7CJi"
   },
   "source": [
    "# Duel network architecture for DRL with prioritized experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all the memory consuming objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvuNN_LhwCww"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SfP00ucD5nL0",
    "outputId": "56c55ca0-bee7-4f91-c2ad-1d0bef8946fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import imageio\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "# tf.image module contains image processing functions\n",
    "# from tensorflow.image import rgb_to_grayscale, resize\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda, Add\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# email\n",
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.multipart import MIMEMultipart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "As0iouS4wGIC"
   },
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBpzT0LL5yS3"
   },
   "outputs": [],
   "source": [
    "## Hyperparameter\n",
    "\n",
    "### change\n",
    "# NUM_EPISODES = 1000000\n",
    "NUM_EPISODES = 5000\n",
    "# NUM_EPISODES = 100\n",
    "# NUM_EPISODES = 4\n",
    "# NUM_EPISODES = 2\n",
    "\n",
    "# DELAY_TRAINING = 50000\n",
    "DELAY_TRAINING = 500\n",
    "# DELAY_TRAINING = 10\n",
    "\n",
    "MODEL_SAVE_FREQ = 500\n",
    "# MODEL_SAVE_FREQ = 10\n",
    "# MODEL_SAVE_FREQ = 2\n",
    "\n",
    "GIF_RECORDING_FREQ = 500\n",
    "# GIF_RECORDING_FREQ = 100\n",
    "# GIF_RECORDING_FREQ = 10\n",
    "# GIF_RECORDING_FREQ = 2\n",
    "\n",
    "EMAIL_MONITORING = 250\n",
    "# EMAIL_MONITORING = 2\n",
    "\n",
    "MEMORY_SIZE = 100000\n",
    "\n",
    "# STORE_PATH = '/content'\n",
    "STORE_PATH = '/home/ubuntu/rl'\n",
    "\n",
    "REWARD_PATH = '/home/ubuntu/rl/duel_total_rewards.pkl'\n",
    "\n",
    "# speed of video (the smaller, the faster)\n",
    "FPS = 30\n",
    "\n",
    "\n",
    "###\n",
    "ENV = 'SpaceInvaders-v0'\n",
    "\n",
    "### prioritized experience replay proportional variant sweet spot values\n",
    "# alpha and beta are from (Schaul et al., 2016) 4 ATARI EXPERIMENTS\n",
    "ALPHA = 0.6\n",
    "# beta value is at the beginning of training, and annealed towards 1 at the end of training\n",
    "BETA = 0.4\n",
    "MIN_PRIORITY = 0.01\n",
    "\n",
    "# (resized height, resized width, gray scale)\n",
    "POST_PROCESS_IMAGE_SIZE = (105, 80, 1)\n",
    "# original image sizes from the Atari Space Invaders\n",
    "PRIOR_H = 210\n",
    "PRIOR_W = 160\n",
    "# color channel\n",
    "PRIOR_C = 3\n",
    "POST_H = 105\n",
    "POST_W = 80\n",
    "POST_C = 1\n",
    "# stacking frames\n",
    "# NUM_FRAMES = 3\n",
    "NUM_FRAMES = 4\n",
    "\n",
    "# get number of actions from action_space.n attribute of env object instead\n",
    "# NUM_ACTIONS = 6\n",
    "\n",
    "# network update from Double Q learning?\n",
    "# TAU = 0.05\n",
    "TAU = 0.08\n",
    "# gif\n",
    "PATH_GIF = \"\"\n",
    "FILE = \"si.gif\"\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_MIN_ITER = 500000\n",
    "\n",
    "MIN_BETA = 0.4\n",
    "MAX_BETA = 1.0\n",
    "BETA_DECAY_ITERS = 500000\n",
    "\n",
    "# email\n",
    "PORT_EMAIL = 465\n",
    "PASSWORD_EMAIL = 'test123TEST!'\n",
    "SENDER_EMAIL = 'airflowkitayama@gmail.com'\n",
    "RECEIVER_EMAIL = 'airflowkitayama@gmail.com'\n",
    "\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_monitoring(episode, reward, avg_loss, epsilon, count, time):\n",
    "    # make contents\n",
    "    subject = 'DL instance RL monitoring during training'\n",
    "    html = \"\"\"\n",
    "    <html>\n",
    "      <p>episode:{:,.0f}, reward:{:,.0f}, avg loss:{:.5f} epsilon:{:.3f}, count:{:,.0f}, time:{:,.1f} from DL instance.</p>\n",
    "    </html>\n",
    "    \"\"\".format(episode, reward, avg_loss, epsilon, count, time)\n",
    "    \n",
    "    # make message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = SENDER_EMAIL\n",
    "    msg['To'] = RECEIVER_EMAIL\n",
    "\n",
    "    # attach content\n",
    "    msgText = MIMEText(html, 'html')\n",
    "    msg.attach(msgText)\n",
    "\n",
    "    # send email\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', PORT_EMAIL, context = context) as server:\n",
    "        server.login(SENDER_EMAIL, PASSWORD_EMAIL)\n",
    "        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFb9mRmnwJHm"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "ScaYHsHG541L",
    "outputId": "c10739ea-6fbc-4e5d-b6c7-c954c3d5daa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actions 6\n",
      "meaning of action space ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "observation space Box(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Environment\n",
    "\n",
    "env = gym.make(ENV)\n",
    "num_actions = env.action_space.n\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# show spec\n",
    "# Discrete(6) but action index includes 0, 1, 2, 3, 4, 5\n",
    "# print(\"action space\", env.action_space)\n",
    "print('number of actions', num_actions)\n",
    "print(\"meaning of action space\", env.get_action_meanings())\n",
    "print(\"observation space\", env.observation_space)\n",
    "\n",
    "# show environment\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSHZaCMLwMGT"
   },
   "source": [
    "## Sum tree data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPREtECxFHRj"
   },
   "outputs": [],
   "source": [
    "## sum-tree data structure\n",
    "\n",
    "# Node class defines information about each node in the tree\n",
    "class Node:\n",
    "    # each of these properties will point to another Node instance\n",
    "    def __init__(self, left, right, is_leaf: bool = False, idx = None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "        # the value property is initialized to be the sum of child left and right nodes\n",
    "        # if not self.is_leaf:\n",
    "        #     self.value = self.left.value + self.right.value\n",
    "        self.value = sum(n.value for n in (left, right) if n is not None)\n",
    "\n",
    "        self.parent = None\n",
    "        self.idx = idx  # this value is only set for leaf nodes\n",
    "        if left is not None:\n",
    "            left.parent = self\n",
    "        if right is not None:\n",
    "            right.parent = self\n",
    "\n",
    "    # cls is the first argument to class methods\n",
    "    # https://www.python.org/dev/peps/pep-0008/#function-and-method-arguments\n",
    "    @classmethod\n",
    "    def create_leaf(cls, value, idx):\n",
    "        # leaf variable is an instance of Node class\n",
    "        leaf = cls(None, None, is_leaf = True, idx = idx)\n",
    "        # leaf weight is set to the passed value\n",
    "        leaf.value = value\n",
    "        return leaf\n",
    "\n",
    "def create_tree(input: list):\n",
    "    \"\"\"\n",
    "    output:\n",
    "        nodes[0] is the top-parent node\n",
    "        leaf_node is the list of leaf nodes\n",
    "    \"\"\"\n",
    "    nodes = [Node.create_leaf(v, i) for i, v in enumerate(input)]\n",
    "    leaf_nodes = nodes\n",
    "    while len(nodes) > 1:\n",
    "        # each time the iterator is called, the element is removed from the iterator\n",
    "        inodes = iter(nodes)\n",
    "        # * used as a parameter to send a non-keyworded variable-length argument list to function\n",
    "        nodes = [Node(*pair) for pair in zip(inodes, inodes)]\n",
    "    return nodes[0], leaf_nodes\n",
    "\n",
    "def retrieve(value: float, node: Node):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        value is a uniformly sampled random value\n",
    "        node is the top-parent node\n",
    "    output:\n",
    "        value of leaf node\n",
    "    \"\"\"\n",
    "    # check if node is lead node\n",
    "    # if true, the sum tree traversal has been completed\n",
    "    if node.is_leaf:\n",
    "        return node\n",
    "    if node.left.value >= value:\n",
    "        return retrieve(value, node.left)\n",
    "    else:\n",
    "        return retrieve(value - node.left.value, node.right)\n",
    "\n",
    "def update(node: Node, new_value: float):\n",
    "    change = new_value - node.value\n",
    "    node.value = new_value\n",
    "    propagate_changes(change, node.parent)\n",
    "\n",
    "def propagate_changes(change: float, node: Node):\n",
    "    node.value += change\n",
    "    if node.parent is not None:\n",
    "        propagate_changes(change, node.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAikZ82AwRBj"
   },
   "source": [
    "## Prioritized experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdTSs_8P8lUu"
   },
   "outputs": [],
   "source": [
    "## prioritized experience replay\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.curr_insert_idx = 0\n",
    "        self.available_samples = 0 # ?\n",
    "        \n",
    "        # what does buffer store? (state, action, reward, terminal)?\n",
    "        # initialize everything in buffer as zero to make sure the size will be able to be maintained during training\n",
    "        # it allows us to check whether it will crash during training because buffer uses memory size\n",
    "        self.buffer = [(np.zeros((POST_H, POST_W), dtype = np.float), 0.0, 0.0, 0.0) for _ in range(self.size)]\n",
    "        \n",
    "        # prioritization parameter in stochastic prioritization\n",
    "        # alpha determines how much prioritization is used, alpha = 0 is uniform\n",
    "        # alpha scales prioritisation based o TD error\n",
    "        # alpha = 0 is uniform ignoring TD error\n",
    "        # alpha = 1 is sampling proportional to TD error\n",
    "        self.alpha = ALPHA\n",
    "        # beta is importance sampling parameter in annealing the bias\n",
    "        # we want to minimise expected value of TD error, but prioritized samping skews or biases the expected value\n",
    "        # importance sampling corrects the bias\n",
    "        self.beta = BETA\n",
    "        # min_priority ensures samples with low TD error still have a small chance of being selected for sampling\n",
    "        self.min_priority = MIN_PRIORITY # add to each experience tuple\n",
    "        \n",
    "        # sum tree algorithm\n",
    "        # sum tree is initialized with the number of leaf nodes equal to the size of the buffer with values 0\n",
    "        self.base_node, self.leaf_nodes = create_tree([0 for i in range(self.size)])\n",
    "        \n",
    "        # location index in experience replay buffer\n",
    "        self.frame_idx = 0\n",
    "        self.action_idx = 1\n",
    "        self.reward_idx = 2\n",
    "        self.terminal_idx = 3\n",
    "\n",
    "    def append(self, experience: tuple, priority: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        experiece : turple\n",
    "        priority : float\n",
    "        \"\"\"\n",
    "        self.buffer[self.curr_insert_idx] = experience\n",
    "        self.update(self.curr_insert_idx, priority)\n",
    "        self.curr_insert_idx += 1\n",
    "        # reset current index if bigger than size\n",
    "        if self.curr_insert_idx >= self.size:\n",
    "            self.curr_insert_idx = 0\n",
    "        # max out available samples at the memory buffer size?\n",
    "        if self.available_samples + 1 < self.size:\n",
    "            self.available_samples += 1\n",
    "        else:\n",
    "            self.available_samples = self.size - 1\n",
    "\n",
    "    # calls the sum tree update function\n",
    "    def update(self, idx: int, priority: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        idx : int\n",
    "        priority : float\n",
    "\n",
    "        below update function is update function of sum tree algorithm helper function\n",
    "        \"\"\"\n",
    "        update(self.leaf_nodes[idx], self.adjust_priority(priority))\n",
    "\n",
    "    # stochastic prioritization (Schaul et al., 2016)\n",
    "    def adjust_priority(self, priority: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        priority : float\n",
    "\n",
    "        Returns\n",
    "        P_(i) = (priority + min_priority)^alphas\n",
    "        \"\"\"\n",
    "        return np.power(priority + self.min_priority, self.alpha)\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        \"\"\"\n",
    "        Perform priority sampling of the experience buffer, \n",
    "        and calculate the importance sampling weights used in training \n",
    "\n",
    "        Parameters\n",
    "        num_samples : int\n",
    "\n",
    "        Returns\n",
    "        states\n",
    "        actions\n",
    "        rewards\n",
    "        next_states\n",
    "        terminal\n",
    "        sampled_idx : list of indices in experience replay buffer by prioritization\n",
    "        is_weights : importance-sampling weights to anneal the bias from prioritization\n",
    "        \"\"\"\n",
    "        sampled_idxs = []\n",
    "        # importance-sampling weights (Schaul et al., 2016)\n",
    "        is_weights = []\n",
    "        sample_no = 0\n",
    "\n",
    "        while sample_no < num_samples:\n",
    "            # draw samples from uniform distribution including lower and excluding upper bound\n",
    "            sample_val = np.random.uniform(0, self.base_node.value)\n",
    "            # use tree sum retrieve function to get weight\n",
    "            samp_node = retrieve(sample_val, self.base_node)\n",
    "\n",
    "            # ?\n",
    "            if NUM_FRAMES - 1 < samp_node.idx < self.available_samples - 1:\n",
    "                sampled_idxs.append(samp_node.idx)\n",
    "                # probability of sampling transition in stochastic prioritization (Schaul et al., 2016)\n",
    "                # base_node.value is sum of all weights because of sum tree\n",
    "                p = samp_node.value / self.base_node.value\n",
    "                # inverse of (1/N) * (1/P(i)) because it powers -beta later\n",
    "                is_weights.append((self.available_samples + 1) * p)\n",
    "                sample_no += 1\n",
    "\n",
    "        # annealing the bias\n",
    "        is_weights = np.array(is_weights)\n",
    "        # power -beta because it did N*P(i) earlier\n",
    "        is_weights = np.power(is_weights, -self.beta)\n",
    "        # normalizing weight only scales the update downwards?\n",
    "        is_weights = is_weights / np.max(is_weights)\n",
    "\n",
    "        # load state and next state according to sampled idxs\n",
    "        # initialize\n",
    "        states = np.zeros((num_samples, POST_H, POST_W, NUM_FRAMES), dtype = np.float32)\n",
    "        next_states = np.zeros((num_samples, POST_H, POST_W, NUM_FRAMES), dtype = np.float32)\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminal = []\n",
    "\n",
    "        for i, idx in enumerate(sampled_idxs):\n",
    "\n",
    "            for j in range(NUM_FRAMES):\n",
    "                # idx + j - NUM_FRAMES + 1(or 2) controls getting stacked images by index drawn from prioritization\n",
    "                states[i, :, :, j] = self.buffer[idx + j - NUM_FRAMES + 1][self.frame_idx][:, :, 0] # 0 at last?\n",
    "                next_states[i, :, :, j] = self.buffer[idx + j - NUM_FRAMES + 2][self.frame_idx][:, :, 0]\n",
    "            actions.append(self.buffer[idx][self.action_idx])\n",
    "            rewards.append(self.buffer[idx][self.reward_idx])\n",
    "            terminal.append(self.buffer[idx][self.terminal_idx])\n",
    "\n",
    "        return states, np.array(actions), np.array(rewards), next_states, np.array(terminal), sampled_idxs, is_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05nkwsKhGnSt"
   },
   "outputs": [],
   "source": [
    "## test sample method of Memory class\n",
    "\n",
    "# sampled_idx = [1, 5, 9]\n",
    "# num_frames = 3\n",
    "\n",
    "# for i, idx in enumerate(sampled_idx):\n",
    "#     for j in range(num_frames):\n",
    "#         state_buffer_idx = idx + j - num_frames + 1\n",
    "#         next_state_buffer_idx = idx + j - num_frames + 2\n",
    "#         print('i', i, 'idx', idx, 'j', j, 'state_buffer_idx', state_buffer_idx, 'next_state_buffer_idx', next_state_buffer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6kD5xq0BkSr"
   },
   "outputs": [],
   "source": [
    "## test prioritized experience replay\n",
    "\n",
    "# 100,000 uses about 7GB of RAM\n",
    "# 1,000,000 will crash\n",
    "# so use 100,000 buffer size\n",
    "\n",
    "# tmp = Memory(100000)\n",
    "# print(len(tmp.buffer))\n",
    "# print(type(tmp.buffer[0]))\n",
    "# print(tmp.buffer[0][0].shape)\n",
    "# print(tmp.buffer[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkhkNJuwwWva"
   },
   "source": [
    "## Dueling network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94Z-IT2R9gsc"
   },
   "outputs": [],
   "source": [
    "## Dueling network architecture\n",
    "\n",
    "### the advantage function expresses the relative benefits of the various actions possible in state s\n",
    "\n",
    "### aggregation layer\n",
    "### the advantage function value is normalized with respect to the mean of the advantage function values over all actions in state s\n",
    "\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, hidden_size: int, num_actions: int, dueling: bool):\n",
    "        super(Network, self).__init__()\n",
    "        self.dueling = dueling\n",
    "        self.conv1 = Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), activation='relu')\n",
    "        self.conv2 = Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        # advantage function A(s, a)\n",
    "        # kernel_initializer?\n",
    "        # he_normal draws samples from a truncated normal distribution centered on 0 with some std\n",
    "        self.adv_dense = Dense(units=hidden_size, activation='relu', kernel_initializer=he_normal())\n",
    "        self.adv_out = Dense(units=num_actions, kernel_initializer=he_normal())\n",
    "        # value function V(s)\n",
    "        if self.dueling:\n",
    "            self.v_dense = Dense(units=hidden_size, activation='relu', kernel_initializer=he_normal())\n",
    "            self.v_out = Dense(units=1, kernel_initializer=he_normal())\n",
    "            # normalized with respect to the mean of the advantage function values over all actions in state s\n",
    "            self.lambda_layer = Lambda(lambda x: x - tf.reduce_mean(x))\n",
    "            # ?\n",
    "            self.combine = Add()\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        # advantage function\n",
    "        adv = self.adv_dense(x)\n",
    "        adv = self.adv_out(adv)\n",
    "        # value function\n",
    "        if self.dueling:\n",
    "            v = self.v_dense(x)\n",
    "            v = self.v_out(v)\n",
    "            norm_adv = self.lambda_layer(adv)\n",
    "            combined = self.combine([v, norm_adv])\n",
    "            return combined\n",
    "        return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXy3K1jGNnCb"
   },
   "outputs": [],
   "source": [
    "## test Network\n",
    "\n",
    "# primary_network = Network(256, num_actions, True)\n",
    "# target_network = Network(256, num_actions, True)\n",
    "\n",
    "# primary_network.compile(optimizer = Adam(), loss = 'mse')\n",
    "\n",
    "# # target_network = primary_network\n",
    "# for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "#     t.assign(e)\n",
    "# target_network.compile(optimizer = Adam(), loss = Huber())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGCJPB-Mwb4h"
   },
   "source": [
    "## Ancillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlqeHPD0Iic0"
   },
   "outputs": [],
   "source": [
    "## Ancillary functions\n",
    "\n",
    "def image_preprocess(image, height = POST_H, width = POST_W):\n",
    "#     image = rgb_to_grayscale(image)\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image, (height, width))\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "def choose_action(state, primary_network, eps, step):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    state\n",
    "    primary_network\n",
    "    eps : epsilon of epsilon greedy algorithm\n",
    "    step : ?\n",
    "    \"\"\"\n",
    "    # ?\n",
    "    if step < DELAY_TRAINING:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(primary_network(tf.reshape(state, (1, POST_H, POST_W, NUM_FRAMES)).numpy()))\n",
    "\n",
    "def update_network(primary_network, target_network):\n",
    "    for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "        t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "# make stacked state\n",
    "def process_state_stack(state_stack, state):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    state_stack : the existing state stack array\n",
    "    state : the most recent state\n",
    "    \"\"\"\n",
    "    # when num frame is 3, most recent state is at index 2\n",
    "    # for loop pushes this state at index 1\n",
    "    for i in range(1, state_stack.shape[-1]):\n",
    "        state_stack[:, :, i - 1].assign(state_stack[:, :, i])\n",
    "    state_stack[:, :, -1].assign(state[:, :, 0])\n",
    "    return state_stack\n",
    "\n",
    "def record_gif(frame_list, episode, fps = FPS):\n",
    "    # imageio.mimsave(PATH_GIF + FILE, frame_list, fps = fps)\n",
    "    # reward? (probably points the reward received in training)\n",
    "    imageio.mimsave(STORE_PATH + '/SPACE_INVADERS_EPISODE-eps{}-r{}.gif'.format(episode, reward), \n",
    "                    frame_list, \n",
    "                    fps = fps)\n",
    "\n",
    "def huber_loss(loss):\n",
    "    return 0.5 * loss ** 2 if abs(loss) < 1.0 else abs(loss) - 0.5\n",
    "\n",
    "def get_per_error(states, actions, rewards, next_states, terminal, primary_network, target_network):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    Returns\n",
    "    target_q\n",
    "    error\n",
    "    \"\"\"\n",
    "    # ?\n",
    "    # predict Q(s, a)\n",
    "    prim_qt = primary_network(states)\n",
    "    # predict Q(s', a')\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "\n",
    "    target_q = prim_qt.numpy()\n",
    "    # action selection from primary (online) network\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis = 1)\n",
    "\n",
    "    # q value for prim_action_tp1 from target network\n",
    "    q_from_target = target_network(next_states)\n",
    "    # if terminal, terminal = 1, so nothing added to reward\n",
    "    updates = rewards + (1 - terminal) * GAMMA * q_from_target.numpy()[:, prim_action_tp1]\n",
    "    target_q[:, actions] = updates\n",
    "\n",
    "    # calculate loss (error) to update priorities\n",
    "    error = [huber_loss( target_q[i, actions[i]] - prim_qt.numpy()[i, actions[i]] ) for i in range(states.shape[0])]\n",
    "\n",
    "    return target_q, error\n",
    "\n",
    "def train(primary_network, memory, target_network):\n",
    "    states, actions, rewards, next_states, terminal, idxs, is_weights = memory.sample(BATCH_SIZE)\n",
    "    target_q, error = get_per_error(states, actions, rewards, next_states, terminal, primary_network, target_network)\n",
    "    for i in range(len(idxs)):\n",
    "        memory.update(idxs[i], error[i])\n",
    "    # use train_on_batch method (x, y, sample_weight) of tensorflow.keras.Model\n",
    "    # is_weights is importance sampling weights\n",
    "    loss = primary_network.train_on_batch(states, target_q, is_weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZjx8550Ku9N"
   },
   "outputs": [],
   "source": [
    "## test ancillary functions\n",
    "\n",
    "### test image_preprocess\n",
    "# image = env.render('rgb_array')\n",
    "# print(type(image), image.shape)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "### for imshow, make it numpy array and remove third color channel 1 input\n",
    "# image_preprocessed = image_preprocess(image, POST_H, POST_W).numpy().reshape((POST_H, POST_W))\n",
    "# print(type(image_preprocessed), image_preprocessed.shape)\n",
    "# plt.imshow(image_preprocessed, cmap = 'gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiQzBFpSwen0"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "colab_type": "code",
    "id": "NnLVoLv3LW_D",
    "outputId": "9fa5499c-6566-422d-e982-3bc80afc0a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: 440, avg loss: -0.29770, epsilon: 0.999, count: 1,275, time : 1.3min\n",
      "recored gif\n",
      "sent email\n",
      "saved models\n",
      "saved rewards\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize replay memory\n",
    "memory = Memory(MEMORY_SIZE)\n",
    "# initialize primary network\n",
    "# Network(number of hidden units in dueling network architecture, number of actions in environment, dueling boolean)\n",
    "primary_network = Network(256, num_actions, True)\n",
    "# primary_network.compile(optimizer = Adam(), loss = 'mse')\n",
    "primary_network.compile(optimizer = Adam(), loss = Huber())\n",
    "# initialize target network\n",
    "target_network = Network(256, num_actions, True)\n",
    "for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "    t.assign(e)\n",
    "# target_network.compile(optimizer = Adam(), loss = Huber())\n",
    "\n",
    "### store rewards, losses, count by initializing np.repeatzero\n",
    "tot_rewards = []\n",
    "\n",
    "num_episodes = NUM_EPISODES\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "# tf.summary.create_file_write creates a summary file writer for the given log directory\n",
    "# train_writer = tf.summary.create_file_writer(STORE_PATH + '/DuelingQPERSI_{}'.format(datetime.now().strftime('%d%m%Y%H%M')))\n",
    "\n",
    "# steps is total steps for all the episodes\n",
    "steps = 0\n",
    "# cnt is steps in each episode\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = image_preprocess(state)\n",
    "    # initialize stack by repeating the same initial states to NUM_FRAMES\n",
    "    state_stack = tf.Variable(np.repeat(state.numpy(), NUM_FRAMES).reshape((POST_H, POST_W, NUM_FRAMES)))\n",
    "    # cnt is a time step in each episode\n",
    "    cnt = 1\n",
    "    avg_loss = 0\n",
    "    tot_reward = 0\n",
    "\n",
    "    if i % GIF_RECORDING_FREQ == 0:\n",
    "        frame_list = []\n",
    "\n",
    "    # while loop will break when done = True\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        action = choose_action(state_stack, primary_network, eps, steps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        tot_reward += reward\n",
    "\n",
    "        if i % GIF_RECORDING_FREQ == 0:\n",
    "            frame_list.append(tf.cast(tf.image.resize(next_state, (480, 320)), tf.uint8).numpy())\n",
    "\n",
    "        next_state = image_preprocess(next_state)\n",
    "        old_state_stack = state_stack\n",
    "        state_stack = process_state_stack(state_stack, next_state)\n",
    "\n",
    "        if steps > DELAY_TRAINING:\n",
    "            loss = train(primary_network, memory, target_network)\n",
    "            update_network(primary_network, target_network)\n",
    "            # no primary_network and target_network arguments?\n",
    "            _, error = get_per_error(tf.reshape(old_state_stack, (1, POST_H, POST_W, NUM_FRAMES)),\n",
    "                                     np.array([action]),\n",
    "                                     np.array([reward]),\n",
    "                                     tf.reshape(state_stack, (1, POST_H, POST_W, NUM_FRAMES)),\n",
    "                                     np.array([done]),\n",
    "                                     primary_network,\n",
    "                                     target_network)\n",
    "            memory.append((next_state, action, reward, done), error[0])\n",
    "        \n",
    "        else:\n",
    "            loss = -1\n",
    "            memory.append((next_state, action, reward, done), reward)\n",
    "\n",
    "        avg_loss += loss\n",
    "\n",
    "        # linearly decay the epsilon and prioritized experience replay beta\n",
    "        if steps > DELAY_TRAINING:\n",
    "            eps = MAX_EPSILON - ((steps - DELAY_TRAINING) / EPSILON_MIN_ITER) * (MAX_EPSILON - MIN_EPSILON) if steps < EPSILON_MIN_ITER else MIN_EPSILON\n",
    "            beta = MIN_BETA + ((steps - DELAY_TRAINING) / BETA_DECAY_ITERS) * (MAX_BETA - MIN_BETA) if steps < BETA_DECAY_ITERS else MAX_BETA\n",
    "            memory.beta = beta\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            if steps > DELAY_TRAINING:\n",
    "                avg_loss /= cnt\n",
    "                print('Episode: {:,.0f}, reward: {:,.0f}, avg loss: {:.5f}, epsilon: {:.3f}, count: {:,.0f}, time : {:.1f}min'.format(i, tot_reward, avg_loss, eps, cnt, (time.time() - start_time) / 60))\n",
    "                # ?\n",
    "#                 with train_writer.as_default():\n",
    "#                     # tf.summary.scalar writes a scalar summary\n",
    "#                     tf.summary.scalar('reward', tot_reward, step = i)\n",
    "#                     tf.summary.scalar('avg loss', avg_loss, step = i)\n",
    "            else:\n",
    "                print('Pre-training...Episode: {}'.format(i))\n",
    "            \n",
    "            if i % GIF_RECORDING_FREQ == 0:\n",
    "                record_gif(frame_list, i, tot_reward)\n",
    "                print('recored gif')\n",
    "                \n",
    "            # save total rewards\n",
    "            tot_rewards.append(tot_reward)\n",
    "            \n",
    "            # break while loop\n",
    "            break\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    # email monitoring\n",
    "    if i % EMAIL_MONITORING == 0:\n",
    "        email_monitoring(i, \n",
    "                         tot_reward, \n",
    "                         avg_loss, \n",
    "                         eps, \n",
    "                         cnt, \n",
    "                         (time.time() - start_time) / 60)\n",
    "        print('sent email')\n",
    "        \n",
    "    if i % MODEL_SAVE_FREQ == 0:\n",
    "        # save_weights method saves all layer weights\n",
    "        primary_network.save_weights(STORE_PATH + '/checkpoints/cp_primary_network_episode_{}.ckpt'.format(i))\n",
    "        target_network.save_weights(STORE_PATH + '/checkpoints/cp_target_network_episode_{}.ckpt'.format(i))\n",
    "        # NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model.\n",
    "        # primary_network.save(STORE_PATH + '/primary_network.h5')\n",
    "        # target_network.save(STORE_PATH + '/target_network.h5')\n",
    "        print('saved models')\n",
    "        # save rewards collected during training\n",
    "        pickle.dump(tot_rewards, open(REWARD_PATH, 'wb'))\n",
    "        print('saved rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send email to notify completing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'RL training finished!'\n",
    "\n",
    "print(subject)\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "  <p>RL training finished!</p>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(html)\n",
    "\n",
    "# make message\n",
    "msg = MIMEMultipart()\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = SENDER_EMAIL\n",
    "msg['To'] = RECEIVER_EMAIL\n",
    "\n",
    "# content\n",
    "msgText = MIMEText(html, 'html')\n",
    "msg.attach(msgText)\n",
    "\n",
    "# send email\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL('smtp.gmail.com', PORT_EMAIL, context = context) as server:\n",
    "    server.login(SENDER_EMAIL, PASSWORD_EMAIL)\n",
    "    server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
